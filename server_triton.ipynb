{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "770966e6-798c-4b64-93c2-cf14d58dfca9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0626 20:56:05.403605 3209 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7fd4c6000000' with size 268435456\n",
      "I0626 20:56:05.403906 3209 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "I0626 20:56:05.404810 3209 model_config_utils.cc:645] Server side auto-completed config: name: \"ensemble_preprocess_fil_postprocess\"\n",
      "platform: \"ensemble\"\n",
      "max_batch_size: 882352\n",
      "input {\n",
      "  name: \"raw_data\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 14\n",
      "}\n",
      "output {\n",
      "  name: \"postprocessed_predictions\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "}\n",
      "ensemble_scheduling {\n",
      "  step {\n",
      "    model_name: \"preprocessing\"\n",
      "    model_version: -1\n",
      "    input_map {\n",
      "      key: \"INPUT\"\n",
      "      value: \"raw_data\"\n",
      "    }\n",
      "    output_map {\n",
      "      key: \"OUTPUT\"\n",
      "      value: \"preprocessed_data\"\n",
      "    }\n",
      "  }\n",
      "  step {\n",
      "    model_name: \"fil\"\n",
      "    model_version: -1\n",
      "    input_map {\n",
      "      key: \"input__0\"\n",
      "      value: \"preprocessed_data\"\n",
      "    }\n",
      "    output_map {\n",
      "      key: \"output__0\"\n",
      "      value: \"predictions\"\n",
      "    }\n",
      "  }\n",
      "  step {\n",
      "    model_name: \"postprocessing\"\n",
      "    model_version: -1\n",
      "    input_map {\n",
      "      key: \"CLASS_IDX\"\n",
      "      value: \"predictions\"\n",
      "    }\n",
      "    output_map {\n",
      "      key: \"CLASS_LABEL\"\n",
      "      value: \"postprocessed_predictions\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "I0626 20:56:05.404946 3209 model_config_utils.cc:645] Server side auto-completed config: name: \"fil\"\n",
      "max_batch_size: 882352\n",
      "input {\n",
      "  name: \"input__0\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 15\n",
      "}\n",
      "output {\n",
      "  name: \"output__0\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "  reshape {\n",
      "  }\n",
      "}\n",
      "instance_group {\n",
      "  kind: KIND_GPU\n",
      "}\n",
      "dynamic_batching {\n",
      "}\n",
      "parameters {\n",
      "  key: \"model_type\"\n",
      "  value {\n",
      "    string_value: \"xgboost_json\"\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  key: \"output_class\"\n",
      "  value {\n",
      "    string_value: \"true\"\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  key: \"storage_type\"\n",
      "  value {\n",
      "    string_value: \"AUTO\"\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  key: \"threshold\"\n",
      "  value {\n",
      "    string_value: \"2\"\n",
      "  }\n",
      "}\n",
      "backend: \"fil\"\n",
      "\n",
      "I0626 20:56:05.405216 3209 model_config_utils.cc:645] Server side auto-completed config: name: \"postprocessing\"\n",
      "max_batch_size: 882352\n",
      "input {\n",
      "  name: \"CLASS_IDX\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "}\n",
      "output {\n",
      "  name: \"CLASS_LABEL\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "}\n",
      "instance_group {\n",
      "  count: 1\n",
      "  kind: KIND_CPU\n",
      "}\n",
      "backend: \"python\"\n",
      "\n",
      "I0626 20:56:05.405466 3209 model_config_utils.cc:645] Server side auto-completed config: name: \"preprocessing\"\n",
      "max_batch_size: 882352\n",
      "input {\n",
      "  name: \"INPUT\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 14\n",
      "}\n",
      "output {\n",
      "  name: \"OUTPUT\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 15\n",
      "}\n",
      "instance_group {\n",
      "  count: 1\n",
      "  kind: KIND_CPU\n",
      "}\n",
      "parameters {\n",
      "  key: \"EXECUTION_ENV_PATH\"\n",
      "  value {\n",
      "    string_value: \"$$TRITON_MODEL_DIRECTORY/rapids22.06_cuda11.5_py3.8.tar.gz\"\n",
      "  }\n",
      "}\n",
      "backend: \"python\"\n",
      "\n",
      "I0626 20:56:05.405681 3209 model_repository_manager.cc:1191] loading: preprocessing:1\n",
      "W0626 20:56:05.505955 3209 model_repository_manager.cc:315] ignore version directory '.ipynb_checkpoints' which fails to convert to integral number\n",
      "I0626 20:56:05.505961 3209 backend_model.cc:292] Adding default backend config setting: default-max-batch-size,4\n",
      "I0626 20:56:05.505998 3209 model_repository_manager.cc:1191] loading: fil:1\n",
      "I0626 20:56:05.506000 3209 shared_library.cc:108] OpenLibraryHandle: /opt/tritonserver/backends/python/libtriton_python.so\n",
      "I0626 20:56:05.507717 3209 python.cc:2144] 'python' TRITONBACKEND API version: 1.9\n",
      "I0626 20:56:05.507731 3209 python.cc:2166] backend configuration:\n",
      "{\"cmdline\":{\"auto-complete-config\":\"false\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}}\n",
      "I0626 20:56:05.507750 3209 python.cc:2296] Shared memory configuration is shm-default-byte-size=67108864,shm-growth-byte-size=67108864,stub-timeout-seconds=30\n",
      "I0626 20:56:05.507887 3209 python.cc:2344] TRITONBACKEND_ModelInitialize: preprocessing (version 1)\n",
      "I0626 20:56:05.508931 3209 model_config_utils.cc:1597] ModelConfig 64-bit fields:\n",
      "I0626 20:56:05.508945 3209 model_config_utils.cc:1599] \tModelConfig::dynamic_batching::default_queue_policy::default_timeout_microseconds\n",
      "I0626 20:56:05.508950 3209 model_config_utils.cc:1599] \tModelConfig::dynamic_batching::max_queue_delay_microseconds\n",
      "I0626 20:56:05.508955 3209 model_config_utils.cc:1599] \tModelConfig::dynamic_batching::priority_queue_policy::value::default_timeout_microseconds\n",
      "I0626 20:56:05.508960 3209 model_config_utils.cc:1599] \tModelConfig::ensemble_scheduling::step::model_version\n",
      "I0626 20:56:05.508966 3209 model_config_utils.cc:1599] \tModelConfig::input::dims\n",
      "I0626 20:56:05.508976 3209 model_config_utils.cc:1599] \tModelConfig::input::reshape::shape\n",
      "I0626 20:56:05.508985 3209 model_config_utils.cc:1599] \tModelConfig::instance_group::secondary_devices::device_id\n",
      "I0626 20:56:05.508994 3209 model_config_utils.cc:1599] \tModelConfig::model_warmup::inputs::value::dims\n",
      "I0626 20:56:05.509002 3209 model_config_utils.cc:1599] \tModelConfig::optimization::cuda::graph_spec::graph_lower_bound::input::value::dim\n",
      "I0626 20:56:05.509009 3209 model_config_utils.cc:1599] \tModelConfig::optimization::cuda::graph_spec::input::value::dim\n",
      "I0626 20:56:05.509014 3209 model_config_utils.cc:1599] \tModelConfig::output::dims\n",
      "I0626 20:56:05.509019 3209 model_config_utils.cc:1599] \tModelConfig::output::reshape::shape\n",
      "I0626 20:56:05.509024 3209 model_config_utils.cc:1599] \tModelConfig::sequence_batching::direct::max_queue_delay_microseconds\n",
      "I0626 20:56:05.509030 3209 model_config_utils.cc:1599] \tModelConfig::sequence_batching::max_sequence_idle_microseconds\n",
      "I0626 20:56:05.509035 3209 model_config_utils.cc:1599] \tModelConfig::sequence_batching::oldest::max_queue_delay_microseconds\n",
      "I0626 20:56:05.509041 3209 model_config_utils.cc:1599] \tModelConfig::sequence_batching::state::dims\n",
      "I0626 20:56:05.509047 3209 model_config_utils.cc:1599] \tModelConfig::sequence_batching::state::initial_state::dims\n",
      "I0626 20:56:05.509053 3209 model_config_utils.cc:1599] \tModelConfig::version_policy::specific::versions\n",
      "I0626 20:56:05.509175 3209 python.cc:2065] Using Python execution env /workspace/triton-serve-fil/preprocessing/rapids22.06_cuda11.5_py3.8.tar.gz\n",
      "I0626 20:56:05.509398 3209 python.cc:2388] TRITONBACKEND_ModelInstanceInitialize: preprocessing_0 (CPU device 0)\n",
      "I0626 20:56:05.509419 3209 backend_model_instance.cc:68] Creating instance preprocessing_0 on CPU using artifact ''\n",
      "W0626 20:56:05.606189 3209 model_repository_manager.cc:315] ignore version directory '.ipynb_checkpoints' which fails to convert to integral number\n",
      "I0626 20:56:05.606215 3209 model_repository_manager.cc:1191] loading: postprocessing:1\n",
      "I0626 20:56:05.606262 3209 backend_model.cc:292] Adding default backend config setting: default-max-batch-size,4\n",
      "I0626 20:56:05.706466 3209 backend_model.cc:292] Adding default backend config setting: default-max-batch-size,4\n",
      "I0626 20:56:33.562891 3256 python.cc:772] Starting Python backend stub: source /tmp/python_env_5Ix8yO/0/bin/activate && exec env LD_LIBRARY_PATH=/tmp/python_env_5Ix8yO/0/lib:$LD_LIBRARY_PATH /opt/tritonserver/backends/python/triton_python_backend_stub /workspace/triton-serve-fil/preprocessing/1/model.py triton_python_backend_shm_region_1 67108864 67108864 3209 /opt/tritonserver/backends/python 336 preprocessing_0\n",
      "I0626 20:56:34.194165 3209 python.cc:2409] TRITONBACKEND_ModelInstanceInitialize: instance initialization successful preprocessing_0 (device 0)\n",
      "I0626 20:56:34.194262 3209 shared_library.cc:108] OpenLibraryHandle: /opt/tritonserver/backends/fil/libtriton_fil.so\n",
      "I0626 20:56:34.194412 3209 backend_model_instance.cc:687] Starting backend thread for preprocessing_0 at nice 0 on device 0...\n",
      "I0626 20:56:34.194583 3209 model_repository_manager.cc:1345] successfully loaded 'preprocessing' version 1\n",
      "I0626 20:56:34.214713 3209 initialize.hpp:43] TRITONBACKEND_Initialize: fil\n",
      "I0626 20:56:34.214734 3209 backend.hpp:47] Triton TRITONBACKEND API version: 1.9\n",
      "I0626 20:56:34.214737 3209 backend.hpp:52] 'fil' TRITONBACKEND API version: 1.9\n",
      "I0626 20:56:34.215017 3209 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: fil (version 1)\n"
     ]
    }
   ],
   "source": [
    "!tritonserver --model-repository `pwd`/triton-serve-fil --log-verbose 1 --exit-on-error false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96dbb7c-6549-46a1-9e8c-c818963e6e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
