{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770966e6-798c-4b64-93c2-cf14d58dfca9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0630 03:45:57.486170 1311 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7fb034000000' with size 268435456\n",
      "I0630 03:45:57.486603 1311 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "I0630 03:45:57.487753 1311 model_config_utils.cc:645] Server side auto-completed config: name: \"ensemble\"\n",
      "platform: \"ensemble\"\n",
      "max_batch_size: 882352\n",
      "input {\n",
      "  name: \"User\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Card\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Year\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Month\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Day\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Time\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Amount\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Use Chip\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Merchant Name\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Merchant City\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Merchant State\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Zip\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"MCC\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Errors?\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "}\n",
      "output {\n",
      "  name: \"predictions\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "}\n",
      "ensemble_scheduling {\n",
      "  step {\n",
      "    model_name: \"preprocessing\"\n",
      "    model_version: 1\n",
      "    input_map {\n",
      "      key: \"Amount\"\n",
      "      value: \"Amount\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"Card\"\n",
      "      value: \"Card\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"Day\"\n",
      "      value: \"Day\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"Errors?\"\n",
      "      value: \"Errors?\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"MCC\"\n",
      "      value: \"MCC\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"Merchant City\"\n",
      "      value: \"Merchant City\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"Merchant Name\"\n",
      "      value: \"Merchant Name\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"Merchant State\"\n",
      "      value: \"Merchant State\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"Month\"\n",
      "      value: \"Month\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"Time\"\n",
      "      value: \"Time\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"Use Chip\"\n",
      "      value: \"Use Chip\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"User\"\n",
      "      value: \"User\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"Year\"\n",
      "      value: \"Year\"\n",
      "    }\n",
      "    input_map {\n",
      "      key: \"Zip\"\n",
      "      value: \"Zip\"\n",
      "    }\n",
      "    output_map {\n",
      "      key: \"OUTPUT\"\n",
      "      value: \"preprocessed_data\"\n",
      "    }\n",
      "  }\n",
      "  step {\n",
      "    model_name: \"fil\"\n",
      "    model_version: 1\n",
      "    input_map {\n",
      "      key: \"input__0\"\n",
      "      value: \"preprocessed_data\"\n",
      "    }\n",
      "    output_map {\n",
      "      key: \"output__0\"\n",
      "      value: \"predictions\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "I0630 03:45:57.487942 1311 model_config_utils.cc:645] Server side auto-completed config: name: \"fil\"\n",
      "max_batch_size: 882352\n",
      "input {\n",
      "  name: \"input__0\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 15\n",
      "}\n",
      "output {\n",
      "  name: \"output__0\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "}\n",
      "instance_group {\n",
      "  kind: KIND_GPU\n",
      "}\n",
      "dynamic_batching {\n",
      "}\n",
      "parameters {\n",
      "  key: \"model_type\"\n",
      "  value {\n",
      "    string_value: \"xgboost_json\"\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  key: \"output_class\"\n",
      "  value {\n",
      "    string_value: \"true\"\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  key: \"predict_proba\"\n",
      "  value {\n",
      "    string_value: \"false\"\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  key: \"storage_type\"\n",
      "  value {\n",
      "    string_value: \"AUTO\"\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  key: \"threshold\"\n",
      "  value {\n",
      "    string_value: \"0.5\"\n",
      "  }\n",
      "}\n",
      "backend: \"fil\"\n",
      "\n",
      "I0630 03:45:57.488256 1311 model_config_utils.cc:645] Server side auto-completed config: name: \"preprocessing\"\n",
      "max_batch_size: 882352\n",
      "input {\n",
      "  name: \"User\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Card\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Year\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Month\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Day\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Time\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Amount\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Use Chip\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Merchant Name\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Merchant City\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Merchant State\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Zip\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"MCC\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "}\n",
      "input {\n",
      "  name: \"Errors?\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "}\n",
      "output {\n",
      "  name: \"OUTPUT\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 15\n",
      "}\n",
      "instance_group {\n",
      "  count: 1\n",
      "  kind: KIND_CPU\n",
      "}\n",
      "parameters {\n",
      "  key: \"EXECUTION_ENV_PATH\"\n",
      "  value {\n",
      "    string_value: \"$$TRITON_MODEL_DIRECTORY/rapids22.06_cuda11.5_py3.8.tar.gz\"\n",
      "  }\n",
      "}\n",
      "backend: \"python\"\n",
      "\n",
      "W0630 03:45:57.488558 1311 model_repository_manager.cc:315] ignore version directory '.ipynb_checkpoints' which fails to convert to integral number\n",
      "I0630 03:45:57.488580 1311 model_repository_manager.cc:1191] loading: preprocessing:1\n",
      "W0630 03:45:57.588854 1311 model_repository_manager.cc:315] ignore version directory '.ipynb_checkpoints' which fails to convert to integral number\n",
      "I0630 03:45:57.588886 1311 backend_model.cc:292] Adding default backend config setting: default-max-batch-size,4\n",
      "I0630 03:45:57.588906 1311 model_repository_manager.cc:1191] loading: fil:1\n",
      "I0630 03:45:57.588926 1311 shared_library.cc:108] OpenLibraryHandle: /opt/tritonserver/backends/python/libtriton_python.so\n",
      "I0630 03:45:57.590554 1311 python.cc:2144] 'python' TRITONBACKEND API version: 1.9\n",
      "I0630 03:45:57.590568 1311 python.cc:2166] backend configuration:\n",
      "{\"cmdline\":{\"auto-complete-config\":\"false\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}}\n",
      "I0630 03:45:57.590594 1311 python.cc:2296] Shared memory configuration is shm-default-byte-size=67108864,shm-growth-byte-size=67108864,stub-timeout-seconds=30\n",
      "I0630 03:45:57.590718 1311 python.cc:2344] TRITONBACKEND_ModelInitialize: preprocessing (version 1)\n",
      "I0630 03:45:57.591937 1311 model_config_utils.cc:1597] ModelConfig 64-bit fields:\n",
      "I0630 03:45:57.591950 1311 model_config_utils.cc:1599] \tModelConfig::dynamic_batching::default_queue_policy::default_timeout_microseconds\n",
      "I0630 03:45:57.591958 1311 model_config_utils.cc:1599] \tModelConfig::dynamic_batching::max_queue_delay_microseconds\n",
      "I0630 03:45:57.591965 1311 model_config_utils.cc:1599] \tModelConfig::dynamic_batching::priority_queue_policy::value::default_timeout_microseconds\n",
      "I0630 03:45:57.591972 1311 model_config_utils.cc:1599] \tModelConfig::ensemble_scheduling::step::model_version\n",
      "I0630 03:45:57.591979 1311 model_config_utils.cc:1599] \tModelConfig::input::dims\n",
      "I0630 03:45:57.591984 1311 model_config_utils.cc:1599] \tModelConfig::input::reshape::shape\n",
      "I0630 03:45:57.591990 1311 model_config_utils.cc:1599] \tModelConfig::instance_group::secondary_devices::device_id\n",
      "I0630 03:45:57.591998 1311 model_config_utils.cc:1599] \tModelConfig::model_warmup::inputs::value::dims\n",
      "I0630 03:45:57.592005 1311 model_config_utils.cc:1599] \tModelConfig::optimization::cuda::graph_spec::graph_lower_bound::input::value::dim\n",
      "I0630 03:45:57.592013 1311 model_config_utils.cc:1599] \tModelConfig::optimization::cuda::graph_spec::input::value::dim\n",
      "I0630 03:45:57.592020 1311 model_config_utils.cc:1599] \tModelConfig::output::dims\n",
      "I0630 03:45:57.592028 1311 model_config_utils.cc:1599] \tModelConfig::output::reshape::shape\n",
      "I0630 03:45:57.592034 1311 model_config_utils.cc:1599] \tModelConfig::sequence_batching::direct::max_queue_delay_microseconds\n",
      "I0630 03:45:57.592044 1311 model_config_utils.cc:1599] \tModelConfig::sequence_batching::max_sequence_idle_microseconds\n",
      "I0630 03:45:57.592050 1311 model_config_utils.cc:1599] \tModelConfig::sequence_batching::oldest::max_queue_delay_microseconds\n",
      "I0630 03:45:57.592057 1311 model_config_utils.cc:1599] \tModelConfig::sequence_batching::state::dims\n",
      "I0630 03:45:57.592063 1311 model_config_utils.cc:1599] \tModelConfig::sequence_batching::state::initial_state::dims\n",
      "I0630 03:45:57.592069 1311 model_config_utils.cc:1599] \tModelConfig::version_policy::specific::versions\n",
      "I0630 03:45:57.592264 1311 python.cc:2065] Using Python execution env /workspace/model_repository/preprocessing/rapids22.06_cuda11.5_py3.8.tar.gz\n",
      "I0630 03:45:57.592505 1311 python.cc:2388] TRITONBACKEND_ModelInstanceInitialize: preprocessing_0 (CPU device 0)\n",
      "I0630 03:45:57.592516 1311 backend_model_instance.cc:68] Creating instance preprocessing_0 on CPU using artifact ''\n",
      "I0630 03:45:57.689206 1311 backend_model.cc:292] Adding default backend config setting: default-max-batch-size,4\n",
      "I0630 03:46:25.550341 1357 python.cc:772] Starting Python backend stub: source /tmp/python_env_z0fDHl/0/bin/activate && exec env LD_LIBRARY_PATH=/tmp/python_env_z0fDHl/0/lib:$LD_LIBRARY_PATH /opt/tritonserver/backends/python/triton_python_backend_stub /workspace/model_repository/preprocessing/1/model.py triton_python_backend_shm_region_1 67108864 67108864 1311 /opt/tritonserver/backends/python 336 preprocessing_0\n",
      "I0630 03:46:26.208434 1311 python.cc:2409] TRITONBACKEND_ModelInstanceInitialize: instance initialization successful preprocessing_0 (device 0)\n",
      "I0630 03:46:26.208518 1311 shared_library.cc:108] OpenLibraryHandle: /opt/tritonserver/backends/fil/libtriton_fil.so\n",
      "I0630 03:46:26.208635 1311 backend_model_instance.cc:687] Starting backend thread for preprocessing_0 at nice 0 on device 0...\n",
      "I0630 03:46:26.208783 1311 model_repository_manager.cc:1345] successfully loaded 'preprocessing' version 1\n",
      "I0630 03:46:26.221491 1311 initialize.hpp:43] TRITONBACKEND_Initialize: fil\n",
      "I0630 03:46:26.221516 1311 backend.hpp:47] Triton TRITONBACKEND API version: 1.9\n",
      "I0630 03:46:26.221525 1311 backend.hpp:52] 'fil' TRITONBACKEND API version: 1.9\n",
      "I0630 03:46:26.221942 1311 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: fil (version 1)\n",
      "I0630 03:46:26.223033 1311 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: fil_0 (GPU device 0)\n",
      "I0630 03:46:26.223161 1311 backend_model_instance.cc:105] Creating instance fil_0 on GPU 0 (8.6) using artifact ''\n",
      "I0630 03:46:26.442990 1311 backend_model_instance.cc:687] Starting backend thread for fil_0 at nice 0 on device 0...\n",
      "I0630 03:46:26.443092 1311 model_repository_manager.cc:1345] successfully loaded 'fil' version 1\n",
      "I0630 03:46:26.443136 1311 dynamic_batch_scheduler.cc:280] Starting dynamic-batcher thread for fil at nice 0...\n",
      "W0630 03:46:26.443433 1311 model_repository_manager.cc:315] ignore version directory '.ipynb_checkpoints' which fails to convert to integral number\n",
      "I0630 03:46:26.443498 1311 model_repository_manager.cc:1191] loading: ensemble:1\n",
      "I0630 03:46:26.543844 1311 ensemble_model.cc:54] ensemble model for ensemble\n",
      "\n",
      "I0630 03:46:26.543879 1311 model_repository_manager.cc:1345] successfully loaded 'ensemble' version 1\n",
      "I0630 03:46:26.543972 1311 server.cc:556] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I0630 03:46:26.544058 1311 server.cc:583] \n",
      "+---------+---------------------------------+---------------------------------+\n",
      "| Backend | Path                            | Config                          |\n",
      "+---------+---------------------------------+---------------------------------+\n",
      "| python  | /opt/tritonserver/backends/pyth | {\"cmdline\":{\"auto-complete-conf |\n",
      "|         | on/libtriton_python.so          | ig\":\"false\",\"min-compute-capabi |\n",
      "|         |                                 | lity\":\"6.000000\",\"backend-direc |\n",
      "|         |                                 | tory\":\"/opt/tritonserver/backen |\n",
      "|         |                                 | ds\",\"default-max-batch-size\":\"4 |\n",
      "|         |                                 | \"}}                             |\n",
      "|         |                                 |                                 |\n",
      "| fil     | /opt/tritonserver/backends/fil/ | {\"cmdline\":{\"auto-complete-conf |\n",
      "|         | libtriton_fil.so                | ig\":\"false\",\"min-compute-capabi |\n",
      "|         |                                 | lity\":\"6.000000\",\"backend-direc |\n",
      "|         |                                 | tory\":\"/opt/tritonserver/backen |\n",
      "|         |                                 | ds\",\"default-max-batch-size\":\"4 |\n",
      "|         |                                 | \"}}                             |\n",
      "|         |                                 |                                 |\n",
      "+---------+---------------------------------+---------------------------------+\n",
      "\n",
      "I0630 03:46:26.544136 1311 server.cc:626] \n",
      "+---------------+---------+--------+\n",
      "| Model         | Version | Status |\n",
      "+---------------+---------+--------+\n",
      "| ensemble      | 1       | READY  |\n",
      "| fil           | 1       | READY  |\n",
      "| preprocessing | 1       | READY  |\n",
      "+---------------+---------+--------+\n",
      "\n",
      "I0630 03:46:26.578167 1311 metrics.cc:650] Collecting metrics for GPU 0: NVIDIA RTX A6000\n",
      "I0630 03:46:26.578405 1311 tritonserver.cc:2138] \n",
      "+----------------------------------+------------------------------------------+\n",
      "| Option                           | Value                                    |\n",
      "+----------------------------------+------------------------------------------+\n",
      "| server_id                        | triton                                   |\n",
      "| server_version                   | 2.22.0                                   |\n",
      "| server_extensions                | classification sequence model_repository |\n",
      "|                                  |  model_repository(unload_dependents) sch |\n",
      "|                                  | edule_policy model_configuration system_ |\n",
      "|                                  | shared_memory cuda_shared_memory binary_ |\n",
      "|                                  | tensor_data statistics trace             |\n",
      "| model_repository_path[0]         | /workspace/model_repository              |\n",
      "| model_control_mode               | MODE_NONE                                |\n",
      "| strict_model_config              | 1                                        |\n",
      "| rate_limit                       | OFF                                      |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                 |\n",
      "| response_cache_byte_size         | 0                                        |\n",
      "| min_supported_compute_capability | 6.0                                      |\n",
      "| strict_readiness                 | 1                                        |\n",
      "| exit_timeout                     | 30                                       |\n",
      "+----------------------------------+------------------------------------------+\n",
      "\n",
      "I0630 03:46:26.578466 1311 grpc_server.cc:4533] === GRPC KeepAlive Options ===\n",
      "I0630 03:46:26.578471 1311 grpc_server.cc:4534] keepalive_time_ms: 7200000\n",
      "I0630 03:46:26.578477 1311 grpc_server.cc:4536] keepalive_timeout_ms: 20000\n",
      "I0630 03:46:26.578479 1311 grpc_server.cc:4538] keepalive_permit_without_calls: 0\n",
      "I0630 03:46:26.578483 1311 grpc_server.cc:4540] http2_max_pings_without_data: 2\n",
      "I0630 03:46:26.578487 1311 grpc_server.cc:4542] http2_min_recv_ping_interval_without_data_ms: 300000\n",
      "I0630 03:46:26.578490 1311 grpc_server.cc:4545] http2_max_ping_strikes: 2\n",
      "I0630 03:46:26.578496 1311 grpc_server.cc:4547] ==============================\n",
      "I0630 03:46:26.578921 1311 grpc_server.cc:225] Ready for RPC 'ServerLive', 0\n",
      "I0630 03:46:26.578954 1311 grpc_server.cc:225] Ready for RPC 'ServerReady', 0\n",
      "I0630 03:46:26.578964 1311 grpc_server.cc:225] Ready for RPC 'ModelReady', 0\n",
      "I0630 03:46:26.578972 1311 grpc_server.cc:225] Ready for RPC 'ServerMetadata', 0\n",
      "I0630 03:46:26.578982 1311 grpc_server.cc:225] Ready for RPC 'ModelMetadata', 0\n",
      "I0630 03:46:26.578991 1311 grpc_server.cc:225] Ready for RPC 'ModelConfig', 0\n",
      "I0630 03:46:26.578998 1311 grpc_server.cc:225] Ready for RPC 'ModelStatistics', 0\n",
      "I0630 03:46:26.579007 1311 grpc_server.cc:225] Ready for RPC 'Trace', 0\n",
      "I0630 03:46:26.579016 1311 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryStatus', 0\n",
      "I0630 03:46:26.579025 1311 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryRegister', 0\n",
      "I0630 03:46:26.579033 1311 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryUnregister', 0\n",
      "I0630 03:46:26.579040 1311 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryStatus', 0\n",
      "I0630 03:46:26.579049 1311 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryRegister', 0\n",
      "I0630 03:46:26.579057 1311 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryUnregister', 0\n",
      "I0630 03:46:26.579064 1311 grpc_server.cc:225] Ready for RPC 'RepositoryIndex', 0\n",
      "I0630 03:46:26.579072 1311 grpc_server.cc:225] Ready for RPC 'RepositoryModelLoad', 0\n",
      "I0630 03:46:26.579080 1311 grpc_server.cc:225] Ready for RPC 'RepositoryModelUnload', 0\n",
      "I0630 03:46:26.579092 1311 grpc_server.cc:419] Thread started for CommonHandler\n",
      "I0630 03:46:26.579147 1311 grpc_server.cc:3587] New request handler for ModelInferHandler, 0\n",
      "I0630 03:46:26.579159 1311 grpc_server.cc:2511] Thread started for ModelInferHandler\n",
      "I0630 03:46:26.579229 1311 grpc_server.cc:3587] New request handler for ModelInferHandler, 0\n",
      "I0630 03:46:26.579279 1311 grpc_server.cc:2511] Thread started for ModelInferHandler\n",
      "I0630 03:46:26.579353 1311 grpc_server.cc:3971] New request handler for ModelStreamInferHandler, 0\n",
      "I0630 03:46:26.579388 1311 grpc_server.cc:2511] Thread started for ModelStreamInferHandler\n",
      "I0630 03:46:26.579394 1311 grpc_server.cc:4589] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I0630 03:46:26.579572 1311 http_server.cc:3303] Started HTTPService at 0.0.0.0:8000\n",
      "I0630 03:46:26.623381 1311 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002\n",
      "I0630 03:46:58.991493 1311 http_server.cc:3203] HTTP request: 2 /v2/models/ensemble/infer\n",
      "I0630 03:46:58.991629 1311 infer_request.cc:710] prepared: [0x0x7fae7c003e70] request id: , model: ensemble, requested version: -1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 5, priority: 0, timeout (us): 0\n",
      "original inputs:\n",
      "[0x0x7fae7c015ff8] input: Errors?, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c014518] input: User, type: FP32, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c014738] input: Card, type: FP32, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c014b58] input: Month, type: FP32, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c014d68] input: Day, type: FP32, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c015bd8] input: Zip, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c014948] input: Year, type: FP32, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c014f78] input: Time, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c015398] input: Use Chip, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c0155a8] input: Merchant Name, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c015188] input: Amount, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c0157b8] input: Merchant City, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c0159c8] input: Merchant State, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c015de8] input: MCC, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "override inputs:\n",
      "inputs:\n",
      "[0x0x7fae7c015de8] input: MCC, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c015ff8] input: Errors?, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c014518] input: User, type: FP32, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c014738] input: Card, type: FP32, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c014b58] input: Month, type: FP32, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c014d68] input: Day, type: FP32, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c015bd8] input: Zip, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c014948] input: Year, type: FP32, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c014f78] input: Time, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c015398] input: Use Chip, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c0155a8] input: Merchant Name, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c015188] input: Amount, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c0157b8] input: Merchant City, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c0159c8] input: Merchant State, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "original requested outputs:\n",
      "requested outputs:\n",
      "predictions\n",
      "\n",
      "I0630 03:46:58.991804 1311 infer_request.cc:710] prepared: [0x0x7fae7c007930] request id: , model: preprocessing, requested version: 1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 5, priority: 0, timeout (us): 0\n",
      "original inputs:\n",
      "[0x0x7fae7c008eb8] input: Merchant State, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c007c88] input: MCC, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c007e38] input: Month, type: FP32, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c007f98] input: Day, type: FP32, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c0080f8] input: Zip, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c008258] input: User, type: FP32, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c0083b8] input: Use Chip, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c008518] input: Time, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c008678] input: Year, type: FP32, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c0087d8] input: Merchant Name, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c008938] input: Amount, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c008a98] input: Merchant City, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c008bf8] input: Errors?, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c008d58] input: Card, type: FP32, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "override inputs:\n",
      "inputs:\n",
      "[0x0x7fae7c008d58] input: Card, type: FP32, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c008eb8] input: Merchant State, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c007c88] input: MCC, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c007e38] input: Month, type: FP32, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c007f98] input: Day, type: FP32, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c0080f8] input: Zip, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c008258] input: User, type: FP32, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c0083b8] input: Use Chip, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c008518] input: Time, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c008678] input: Year, type: FP32, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c0087d8] input: Merchant Name, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c008938] input: Amount, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c008a98] input: Merchant City, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "[0x0x7fae7c008bf8] input: Errors?, type: BYTES, original shape: [5,1], batch + shape: [5,1], shape: [1]\n",
      "original requested outputs:\n",
      "OUTPUT\n",
      "requested outputs:\n",
      "OUTPUT\n",
      "\n",
      "I0630 03:46:58.991964 1311 python.cc:1553] model preprocessing, instance preprocessing_0, executing 1 requests\n",
      "I0630 03:46:59.059686 1311 infer_response.cc:167] add response output: output: OUTPUT, type: FP32, shape: [5,15]\n",
      "I0630 03:46:59.059714 1311 pinned_memory_manager.cc:161] pinned memory allocation: size 300, addr 0x7fb034000090\n",
      "I0630 03:46:59.059723 1311 ensemble_scheduler.cc:540] Internal response allocation: OUTPUT, size 300, addr 0x7fb034000090, memory type 1, type id 0\n",
      "I0630 03:46:59.059750 1311 ensemble_scheduler.cc:555] Internal response release: size 300, addr 0x7fb034000090\n",
      "I0630 03:46:59.059766 1311 infer_request.cc:710] prepared: [0x0x7faf880069a0] request id: , model: fil, requested version: 1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 5, priority: 0, timeout (us): 0\n",
      "original inputs:\n",
      "[0x0x7faf88006cd8] input: input__0, type: FP32, original shape: [5,15], batch + shape: [5,15], shape: [15]\n",
      "override inputs:\n",
      "inputs:\n",
      "[0x0x7faf88006cd8] input: input__0, type: FP32, original shape: [5,15], batch + shape: [5,15], shape: [15]\n",
      "original requested outputs:\n",
      "output__0\n",
      "requested outputs:\n",
      "output__0\n",
      "\n",
      "I0630 03:46:59.059816 1311 python.cc:2491] TRITONBACKEND_ModelInstanceExecute: model instance name preprocessing_0 released 1 requests\n",
      "I0630 03:46:59.065652 1311 infer_response.cc:167] add response output: output: output__0, type: FP32, shape: [5]\n",
      "I0630 03:46:59.065672 1311 ensemble_scheduler.cc:540] Internal response allocation: output__0, size 20, addr 0x7fb0307cc800, memory type 2, type id 0\n",
      "I0630 03:46:59.065706 1311 ensemble_scheduler.cc:555] Internal response release: size 20, addr 0x7fb0307cc800\n",
      "I0630 03:46:59.065717 1311 infer_response.cc:141] add response output: output: predictions, type: FP32, shape: [5]\n",
      "I0630 03:46:59.065726 1311 http_server.cc:1088] HTTP: unable to provide 'predictions' in GPU, will use CPU\n",
      "I0630 03:46:59.065735 1311 http_server.cc:1108] HTTP using buffer for: 'predictions', size: 20, addr: 0x7faed00072c0\n",
      "I0630 03:46:59.065801 1311 http_server.cc:1182] HTTP release: size 20, addr 0x7faed00072c0\n",
      "I0630 03:46:59.065815 1311 pinned_memory_manager.cc:190] pinned memory deallocation: addr 0x7fb034000090\n"
     ]
    }
   ],
   "source": [
    "!tritonserver --model-repository `pwd`/model_repository --log-verbose 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96dbb7c-6549-46a1-9e8c-c818963e6e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
