{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "770966e6-798c-4b64-93c2-cf14d58dfca9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0626 07:53:18.603357 4168 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7fba06000000' with size 268435456\n",
      "I0626 07:53:18.603651 4168 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "I0626 07:53:18.604571 4168 model_config_utils.cc:645] Server side auto-completed config: name: \"preprocessing\"\n",
      "max_batch_size: 8\n",
      "input {\n",
      "  name: \"INPUT\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 14\n",
      "}\n",
      "output {\n",
      "  name: \"OUTPUT\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 15\n",
      "}\n",
      "instance_group {\n",
      "  count: 1\n",
      "  kind: KIND_GPU\n",
      "}\n",
      "parameters {\n",
      "  key: \"EXECUTION_ENV_PATH\"\n",
      "  value {\n",
      "    string_value: \"$$TRITON_MODEL_DIRECTORY/rapids22.06_cuda11.5_py3.8.tar.gz\"\n",
      "  }\n",
      "}\n",
      "backend: \"python\"\n",
      "\n",
      "I0626 07:53:18.604768 4168 model_repository_manager.cc:898] AsyncLoad() 'preprocessing'\n",
      "W0626 07:53:18.604839 4168 model_repository_manager.cc:315] ignore version directory '.ipynb_checkpoints' which fails to convert to integral number\n",
      "I0626 07:53:18.604857 4168 model_repository_manager.cc:1136] TriggerNextAction() 'preprocessing' version 1: 1\n",
      "I0626 07:53:18.604861 4168 model_repository_manager.cc:1172] Load() 'preprocessing' version 1\n",
      "I0626 07:53:18.604864 4168 model_repository_manager.cc:1191] loading: preprocessing:1\n",
      "I0626 07:53:18.705104 4168 model_repository_manager.cc:1249] CreateModel() 'preprocessing' version 1\n",
      "I0626 07:53:18.705213 4168 backend_model.cc:292] Adding default backend config setting: default-max-batch-size,4\n",
      "I0626 07:53:18.705234 4168 shared_library.cc:108] OpenLibraryHandle: /opt/tritonserver/backends/python/libtriton_python.so\n",
      "I0626 07:53:18.707016 4168 python.cc:2144] 'python' TRITONBACKEND API version: 1.9\n",
      "I0626 07:53:18.707032 4168 python.cc:2166] backend configuration:\n",
      "{\"cmdline\":{\"auto-complete-config\":\"false\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}}\n",
      "I0626 07:53:18.707056 4168 python.cc:2296] Shared memory configuration is shm-default-byte-size=67108864,shm-growth-byte-size=67108864,stub-timeout-seconds=30\n",
      "I0626 07:53:18.707198 4168 python.cc:2344] TRITONBACKEND_ModelInitialize: preprocessing (version 1)\n",
      "I0626 07:53:18.708397 4168 model_config_utils.cc:1597] ModelConfig 64-bit fields:\n",
      "I0626 07:53:18.708412 4168 model_config_utils.cc:1599] \tModelConfig::dynamic_batching::default_queue_policy::default_timeout_microseconds\n",
      "I0626 07:53:18.708426 4168 model_config_utils.cc:1599] \tModelConfig::dynamic_batching::max_queue_delay_microseconds\n",
      "I0626 07:53:18.708436 4168 model_config_utils.cc:1599] \tModelConfig::dynamic_batching::priority_queue_policy::value::default_timeout_microseconds\n",
      "I0626 07:53:18.708446 4168 model_config_utils.cc:1599] \tModelConfig::ensemble_scheduling::step::model_version\n",
      "I0626 07:53:18.708454 4168 model_config_utils.cc:1599] \tModelConfig::input::dims\n",
      "I0626 07:53:18.708471 4168 model_config_utils.cc:1599] \tModelConfig::input::reshape::shape\n",
      "I0626 07:53:18.708478 4168 model_config_utils.cc:1599] \tModelConfig::instance_group::secondary_devices::device_id\n",
      "I0626 07:53:18.708485 4168 model_config_utils.cc:1599] \tModelConfig::model_warmup::inputs::value::dims\n",
      "I0626 07:53:18.708493 4168 model_config_utils.cc:1599] \tModelConfig::optimization::cuda::graph_spec::graph_lower_bound::input::value::dim\n",
      "I0626 07:53:18.708501 4168 model_config_utils.cc:1599] \tModelConfig::optimization::cuda::graph_spec::input::value::dim\n",
      "I0626 07:53:18.708508 4168 model_config_utils.cc:1599] \tModelConfig::output::dims\n",
      "I0626 07:53:18.708515 4168 model_config_utils.cc:1599] \tModelConfig::output::reshape::shape\n",
      "I0626 07:53:18.708522 4168 model_config_utils.cc:1599] \tModelConfig::sequence_batching::direct::max_queue_delay_microseconds\n",
      "I0626 07:53:18.708529 4168 model_config_utils.cc:1599] \tModelConfig::sequence_batching::max_sequence_idle_microseconds\n",
      "I0626 07:53:18.708536 4168 model_config_utils.cc:1599] \tModelConfig::sequence_batching::oldest::max_queue_delay_microseconds\n",
      "I0626 07:53:18.708544 4168 model_config_utils.cc:1599] \tModelConfig::sequence_batching::state::dims\n",
      "I0626 07:53:18.708551 4168 model_config_utils.cc:1599] \tModelConfig::sequence_batching::state::initial_state::dims\n",
      "I0626 07:53:18.708558 4168 model_config_utils.cc:1599] \tModelConfig::version_policy::specific::versions\n",
      "I0626 07:53:18.708700 4168 python.cc:2065] Using Python execution env /workspace/triton-serve-fil/preprocessing/rapids22.06_cuda11.5_py3.8.tar.gz\n",
      "I0626 07:53:18.790394 4168 python.cc:2388] TRITONBACKEND_ModelInstanceInitialize: preprocessing_0 (GPU device 0)\n",
      "I0626 07:53:18.790548 4168 backend_model_instance.cc:105] Creating instance preprocessing_0 on GPU 0 (8.6) using artifact ''\n",
      "I0626 07:53:47.183395 4213 python.cc:772] Starting Python backend stub: source /tmp/python_env_PMzyJl/0/bin/activate && exec env LD_LIBRARY_PATH=/tmp/python_env_PMzyJl/0/lib:$LD_LIBRARY_PATH /opt/tritonserver/backends/python/triton_python_backend_stub /workspace/triton-serve-fil/preprocessing/1/model.py triton_python_backend_shm_region_1 67108864 67108864 4168 /opt/tritonserver/backends/python 336 preprocessing_0\n",
      "I0626 07:53:47.835095 4168 python.cc:2409] TRITONBACKEND_ModelInstanceInitialize: instance initialization successful preprocessing_0 (device 0)\n",
      "I0626 07:53:47.835309 4168 backend_model_instance.cc:687] Starting backend thread for preprocessing_0 at nice 0 on device 0...\n",
      "I0626 07:53:47.835443 4168 model_repository_manager.cc:1345] successfully loaded 'preprocessing' version 1\n",
      "I0626 07:53:47.835454 4168 model_repository_manager.cc:1136] TriggerNextAction() 'preprocessing' version 1: 0\n",
      "I0626 07:53:47.835461 4168 model_repository_manager.cc:1150] no next action, trigger OnComplete()\n",
      "I0626 07:53:47.835490 4168 model_repository_manager.cc:713] VersionStates() 'preprocessing'\n",
      "I0626 07:53:47.835512 4168 model_repository_manager.cc:713] VersionStates() 'preprocessing'\n",
      "I0626 07:53:47.835536 4168 server.cc:556] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I0626 07:53:47.835571 4168 server.cc:583] \n",
      "+---------+---------------------------------+---------------------------------+\n",
      "| Backend | Path                            | Config                          |\n",
      "+---------+---------------------------------+---------------------------------+\n",
      "| python  | /opt/tritonserver/backends/pyth | {\"cmdline\":{\"auto-complete-conf |\n",
      "|         | on/libtriton_python.so          | ig\":\"false\",\"min-compute-capabi |\n",
      "|         |                                 | lity\":\"6.000000\",\"backend-direc |\n",
      "|         |                                 | tory\":\"/opt/tritonserver/backen |\n",
      "|         |                                 | ds\",\"default-max-batch-size\":\"4 |\n",
      "|         |                                 | \"}}                             |\n",
      "|         |                                 |                                 |\n",
      "+---------+---------------------------------+---------------------------------+\n",
      "\n",
      "I0626 07:53:47.835586 4168 model_repository_manager.cc:689] ModelStates()\n",
      "I0626 07:53:47.835602 4168 server.cc:626] \n",
      "+---------------+---------+--------+\n",
      "| Model         | Version | Status |\n",
      "+---------------+---------+--------+\n",
      "| preprocessing | 1       | READY  |\n",
      "+---------------+---------+--------+\n",
      "\n",
      "I0626 07:53:47.864329 4168 metrics.cc:650] Collecting metrics for GPU 0: NVIDIA RTX A6000\n",
      "I0626 07:53:47.864587 4168 tritonserver.cc:2138] \n",
      "+----------------------------------+------------------------------------------+\n",
      "| Option                           | Value                                    |\n",
      "+----------------------------------+------------------------------------------+\n",
      "| server_id                        | triton                                   |\n",
      "| server_version                   | 2.22.0                                   |\n",
      "| server_extensions                | classification sequence model_repository |\n",
      "|                                  |  model_repository(unload_dependents) sch |\n",
      "|                                  | edule_policy model_configuration system_ |\n",
      "|                                  | shared_memory cuda_shared_memory binary_ |\n",
      "|                                  | tensor_data statistics trace             |\n",
      "| model_repository_path[0]         | /workspace/triton-serve-fil              |\n",
      "| model_control_mode               | MODE_NONE                                |\n",
      "| strict_model_config              | 1                                        |\n",
      "| rate_limit                       | OFF                                      |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                 |\n",
      "| response_cache_byte_size         | 0                                        |\n",
      "| min_supported_compute_capability | 6.0                                      |\n",
      "| strict_readiness                 | 1                                        |\n",
      "| exit_timeout                     | 30                                       |\n",
      "+----------------------------------+------------------------------------------+\n",
      "\n",
      "I0626 07:53:47.864655 4168 grpc_server.cc:4533] === GRPC KeepAlive Options ===\n",
      "I0626 07:53:47.864660 4168 grpc_server.cc:4534] keepalive_time_ms: 7200000\n",
      "I0626 07:53:47.864665 4168 grpc_server.cc:4536] keepalive_timeout_ms: 20000\n",
      "I0626 07:53:47.864669 4168 grpc_server.cc:4538] keepalive_permit_without_calls: 0\n",
      "I0626 07:53:47.864673 4168 grpc_server.cc:4540] http2_max_pings_without_data: 2\n",
      "I0626 07:53:47.864676 4168 grpc_server.cc:4542] http2_min_recv_ping_interval_without_data_ms: 300000\n",
      "I0626 07:53:47.864680 4168 grpc_server.cc:4545] http2_max_ping_strikes: 2\n",
      "I0626 07:53:47.864683 4168 grpc_server.cc:4547] ==============================\n",
      "I0626 07:53:47.865168 4168 grpc_server.cc:225] Ready for RPC 'ServerLive', 0\n",
      "I0626 07:53:47.865196 4168 grpc_server.cc:225] Ready for RPC 'ServerReady', 0\n",
      "I0626 07:53:47.865207 4168 grpc_server.cc:225] Ready for RPC 'ModelReady', 0\n",
      "I0626 07:53:47.865214 4168 grpc_server.cc:225] Ready for RPC 'ServerMetadata', 0\n",
      "I0626 07:53:47.865223 4168 grpc_server.cc:225] Ready for RPC 'ModelMetadata', 0\n",
      "I0626 07:53:47.865232 4168 grpc_server.cc:225] Ready for RPC 'ModelConfig', 0\n",
      "I0626 07:53:47.865239 4168 grpc_server.cc:225] Ready for RPC 'ModelStatistics', 0\n",
      "I0626 07:53:47.865249 4168 grpc_server.cc:225] Ready for RPC 'Trace', 0\n",
      "I0626 07:53:47.865257 4168 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryStatus', 0\n",
      "I0626 07:53:47.865266 4168 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryRegister', 0\n",
      "I0626 07:53:47.865274 4168 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryUnregister', 0\n",
      "I0626 07:53:47.865281 4168 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryStatus', 0\n",
      "I0626 07:53:47.865289 4168 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryRegister', 0\n",
      "I0626 07:53:47.865297 4168 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryUnregister', 0\n",
      "I0626 07:53:47.865304 4168 grpc_server.cc:225] Ready for RPC 'RepositoryIndex', 0\n",
      "I0626 07:53:47.865312 4168 grpc_server.cc:225] Ready for RPC 'RepositoryModelLoad', 0\n",
      "I0626 07:53:47.865322 4168 grpc_server.cc:225] Ready for RPC 'RepositoryModelUnload', 0\n",
      "I0626 07:53:47.865333 4168 grpc_server.cc:419] Thread started for CommonHandler\n",
      "I0626 07:53:47.865393 4168 grpc_server.cc:3587] New request handler for ModelInferHandler, 0\n",
      "I0626 07:53:47.865407 4168 grpc_server.cc:2511] Thread started for ModelInferHandler\n",
      "I0626 07:53:47.865453 4168 grpc_server.cc:3587] New request handler for ModelInferHandler, 0\n",
      "I0626 07:53:47.865468 4168 grpc_server.cc:2511] Thread started for ModelInferHandler\n",
      "I0626 07:53:47.865512 4168 grpc_server.cc:3971] New request handler for ModelStreamInferHandler, 0\n",
      "I0626 07:53:47.865526 4168 grpc_server.cc:2511] Thread started for ModelStreamInferHandler\n",
      "I0626 07:53:47.865532 4168 grpc_server.cc:4589] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I0626 07:53:47.865718 4168 http_server.cc:3303] Started HTTPService at 0.0.0.0:8000\n",
      "I0626 07:53:47.913222 4168 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002\n",
      "I0626 07:54:07.772880 4168 http_server.cc:3203] HTTP request: 2 /v2/models/preprocessing/infer\n",
      "I0626 07:54:07.772924 4168 model_repository_manager.cc:758] GetModel() 'preprocessing' version -1\n",
      "I0626 07:54:07.772940 4168 model_repository_manager.cc:758] GetModel() 'preprocessing' version -1\n",
      "I0626 07:54:07.772986 4168 http_server.cc:2740] Infer failed: inference request batch-size must be <= 8 for 'preprocessing'\n",
      "I0626 07:54:42.284599 4168 http_server.cc:3203] HTTP request: 2 /v2/models/preprocessing/infer\n",
      "I0626 07:54:42.284641 4168 model_repository_manager.cc:758] GetModel() 'preprocessing' version -1\n",
      "I0626 07:54:42.284656 4168 model_repository_manager.cc:758] GetModel() 'preprocessing' version -1\n",
      "I0626 07:54:42.284697 4168 infer_request.cc:710] prepared: [0x0x7fb92c002f00] request id: , model: preprocessing, requested version: -1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 2, priority: 0, timeout (us): 0\n",
      "original inputs:\n",
      "[0x0x7fb92c013478] input: INPUT, type: BYTES, original shape: [2,14], batch + shape: [2,14], shape: [14]\n",
      "override inputs:\n",
      "inputs:\n",
      "[0x0x7fb92c013478] input: INPUT, type: BYTES, original shape: [2,14], batch + shape: [2,14], shape: [14]\n",
      "original requested outputs:\n",
      "requested outputs:\n",
      "OUTPUT\n",
      "\n",
      "I0626 07:54:42.284798 4168 python.cc:1553] model preprocessing, instance preprocessing_0, executing 1 requests\n",
      "input data is [[b'1005' b'0' b'2013' b'6' b'5' b'22:03' b'$-180.00'\n",
      "  b'Swipe Transaction' b'7834055923142137930' b'Orlando' b'FL' b'32808.0'\n",
      "  b'3395' b'nan']\n",
      " [b'134' b'5' b'2015' b'2' b'16' b'18:34' b'$99.10' b'Chip Transaction'\n",
      "  b'-1548923525906069124' b'Tyler' b'TX' b'75706.0' b'4900' b'nan']]\n",
      "type of input_data is <class 'numpy.ndarray'>\n",
      "0626 07:54:42.285616 4213 pb_stub.cc:749] Failed to process the request(s) for model 'preprocessing_0', message: Number of InferenceResponse objects do not match the number of InferenceRequest objects. InferenceRequest(s) size is:1, and InferenceResponse(s) size is:0\n",
      "\n",
      "I0626 07:54:42.285745 4168 python.cc:2491] TRITONBACKEND_ModelInstanceExecute: model instance name preprocessing_0 released 1 requests\n"
     ]
    }
   ],
   "source": [
    "!tritonserver --model-repository `pwd`/triton-serve-fil --log-verbose 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96dbb7c-6549-46a1-9e8c-c818963e6e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
