{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "770966e6-798c-4b64-93c2-cf14d58dfca9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0626 22:19:23.040475 3220 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7f5c50000000' with size 268435456\n",
      "I0626 22:19:23.040781 3220 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "I0626 22:19:23.041678 3220 model_config_utils.cc:645] Server side auto-completed config: name: \"ensemble_preprocess_fil_postprocess\"\n",
      "platform: \"ensemble\"\n",
      "max_batch_size: 882352\n",
      "input {\n",
      "  name: \"raw_data\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 14\n",
      "}\n",
      "output {\n",
      "  name: \"processed_predictions\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "}\n",
      "ensemble_scheduling {\n",
      "  step {\n",
      "    model_name: \"preprocessing\"\n",
      "    model_version: -1\n",
      "    input_map {\n",
      "      key: \"INPUT\"\n",
      "      value: \"raw_data\"\n",
      "    }\n",
      "    output_map {\n",
      "      key: \"OUTPUT\"\n",
      "      value: \"preprocessed_data\"\n",
      "    }\n",
      "  }\n",
      "  step {\n",
      "    model_name: \"fil\"\n",
      "    model_version: -1\n",
      "    input_map {\n",
      "      key: \"input__0\"\n",
      "      value: \"preprocessed_data\"\n",
      "    }\n",
      "    output_map {\n",
      "      key: \"output__0\"\n",
      "      value: \"predictions\"\n",
      "    }\n",
      "  }\n",
      "  step {\n",
      "    model_name: \"postprocessing\"\n",
      "    model_version: -1\n",
      "    input_map {\n",
      "      key: \"CLASS_IDX\"\n",
      "      value: \"predictions\"\n",
      "    }\n",
      "    output_map {\n",
      "      key: \"CLASS_LABEL\"\n",
      "      value: \"processed_predictions\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "I0626 22:19:23.041814 3220 model_config_utils.cc:645] Server side auto-completed config: name: \"fil\"\n",
      "max_batch_size: 882352\n",
      "input {\n",
      "  name: \"input__0\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 15\n",
      "}\n",
      "output {\n",
      "  name: \"output__0\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "}\n",
      "instance_group {\n",
      "  kind: KIND_GPU\n",
      "}\n",
      "dynamic_batching {\n",
      "}\n",
      "parameters {\n",
      "  key: \"model_type\"\n",
      "  value {\n",
      "    string_value: \"xgboost_json\"\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  key: \"output_class\"\n",
      "  value {\n",
      "    string_value: \"true\"\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  key: \"storage_type\"\n",
      "  value {\n",
      "    string_value: \"AUTO\"\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  key: \"threshold\"\n",
      "  value {\n",
      "    string_value: \"0.5\"\n",
      "  }\n",
      "}\n",
      "backend: \"fil\"\n",
      "\n",
      "I0626 22:19:23.042076 3220 model_config_utils.cc:645] Server side auto-completed config: name: \"postprocessing\"\n",
      "max_batch_size: 882352\n",
      "input {\n",
      "  name: \"CLASS_IDX\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 1\n",
      "  reshape {\n",
      "  }\n",
      "}\n",
      "output {\n",
      "  name: \"CLASS_LABEL\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "}\n",
      "instance_group {\n",
      "  count: 1\n",
      "  kind: KIND_CPU\n",
      "}\n",
      "backend: \"python\"\n",
      "\n",
      "I0626 22:19:23.042317 3220 model_config_utils.cc:645] Server side auto-completed config: name: \"preprocessing\"\n",
      "max_batch_size: 882352\n",
      "input {\n",
      "  name: \"INPUT\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 14\n",
      "}\n",
      "output {\n",
      "  name: \"OUTPUT\"\n",
      "  data_type: TYPE_FP32\n",
      "  dims: 15\n",
      "}\n",
      "instance_group {\n",
      "  count: 1\n",
      "  kind: KIND_CPU\n",
      "}\n",
      "parameters {\n",
      "  key: \"EXECUTION_ENV_PATH\"\n",
      "  value {\n",
      "    string_value: \"$$TRITON_MODEL_DIRECTORY/rapids22.06_cuda11.5_py3.8.tar.gz\"\n",
      "  }\n",
      "}\n",
      "backend: \"python\"\n",
      "\n",
      "I0626 22:19:23.042532 3220 model_repository_manager.cc:1191] loading: fil:1\n",
      "I0626 22:19:23.142699 3220 model_repository_manager.cc:1191] loading: postprocessing:1\n",
      "I0626 22:19:23.142825 3220 backend_model.cc:292] Adding default backend config setting: default-max-batch-size,4\n",
      "I0626 22:19:23.142871 3220 shared_library.cc:108] OpenLibraryHandle: /opt/tritonserver/backends/fil/libtriton_fil.so\n",
      "I0626 22:19:23.157046 3220 initialize.hpp:43] TRITONBACKEND_Initialize: fil\n",
      "I0626 22:19:23.157063 3220 backend.hpp:47] Triton TRITONBACKEND API version: 1.9\n",
      "I0626 22:19:23.157066 3220 backend.hpp:52] 'fil' TRITONBACKEND API version: 1.9\n",
      "I0626 22:19:23.157332 3220 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: fil (version 1)\n",
      "I0626 22:19:23.157992 3220 model_config_utils.cc:1597] ModelConfig 64-bit fields:\n",
      "I0626 22:19:23.157999 3220 model_config_utils.cc:1599] \tModelConfig::dynamic_batching::default_queue_policy::default_timeout_microseconds\n",
      "I0626 22:19:23.158003 3220 model_config_utils.cc:1599] \tModelConfig::dynamic_batching::max_queue_delay_microseconds\n",
      "I0626 22:19:23.158005 3220 model_config_utils.cc:1599] \tModelConfig::dynamic_batching::priority_queue_policy::value::default_timeout_microseconds\n",
      "I0626 22:19:23.158009 3220 model_config_utils.cc:1599] \tModelConfig::ensemble_scheduling::step::model_version\n",
      "I0626 22:19:23.158011 3220 model_config_utils.cc:1599] \tModelConfig::input::dims\n",
      "I0626 22:19:23.158013 3220 model_config_utils.cc:1599] \tModelConfig::input::reshape::shape\n",
      "I0626 22:19:23.158016 3220 model_config_utils.cc:1599] \tModelConfig::instance_group::secondary_devices::device_id\n",
      "I0626 22:19:23.158018 3220 model_config_utils.cc:1599] \tModelConfig::model_warmup::inputs::value::dims\n",
      "I0626 22:19:23.158023 3220 model_config_utils.cc:1599] \tModelConfig::optimization::cuda::graph_spec::graph_lower_bound::input::value::dim\n",
      "I0626 22:19:23.158029 3220 model_config_utils.cc:1599] \tModelConfig::optimization::cuda::graph_spec::input::value::dim\n",
      "I0626 22:19:23.158034 3220 model_config_utils.cc:1599] \tModelConfig::output::dims\n",
      "I0626 22:19:23.158038 3220 model_config_utils.cc:1599] \tModelConfig::output::reshape::shape\n",
      "I0626 22:19:23.158040 3220 model_config_utils.cc:1599] \tModelConfig::sequence_batching::direct::max_queue_delay_microseconds\n",
      "I0626 22:19:23.158045 3220 model_config_utils.cc:1599] \tModelConfig::sequence_batching::max_sequence_idle_microseconds\n",
      "I0626 22:19:23.158048 3220 model_config_utils.cc:1599] \tModelConfig::sequence_batching::oldest::max_queue_delay_microseconds\n",
      "I0626 22:19:23.158054 3220 model_config_utils.cc:1599] \tModelConfig::sequence_batching::state::dims\n",
      "I0626 22:19:23.158057 3220 model_config_utils.cc:1599] \tModelConfig::sequence_batching::state::initial_state::dims\n",
      "I0626 22:19:23.158061 3220 model_config_utils.cc:1599] \tModelConfig::version_policy::specific::versions\n",
      "I0626 22:19:23.158473 3220 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: fil_0 (GPU device 0)\n",
      "I0626 22:19:23.158556 3220 backend_model_instance.cc:105] Creating instance fil_0 on GPU 0 (8.6) using artifact ''\n",
      "I0626 22:19:23.244279 3220 model_repository_manager.cc:1191] loading: preprocessing:1\n",
      "I0626 22:19:23.244338 3220 backend_model.cc:292] Adding default backend config setting: default-max-batch-size,4\n",
      "I0626 22:19:23.252050 3220 shared_library.cc:108] OpenLibraryHandle: /opt/tritonserver/backends/python/libtriton_python.so\n",
      "I0626 22:19:23.252152 3220 backend_model_instance.cc:687] Starting backend thread for fil_0 at nice 0 on device 0...\n",
      "I0626 22:19:23.252257 3220 model_repository_manager.cc:1345] successfully loaded 'fil' version 1\n",
      "I0626 22:19:23.252302 3220 dynamic_batch_scheduler.cc:280] Starting dynamic-batcher thread for fil at nice 0...\n",
      "I0626 22:19:23.253599 3220 python.cc:2144] 'python' TRITONBACKEND API version: 1.9\n",
      "I0626 22:19:23.253615 3220 python.cc:2166] backend configuration:\n",
      "{\"cmdline\":{\"auto-complete-config\":\"false\",\"min-compute-capability\":\"6.000000\",\"backend-directory\":\"/opt/tritonserver/backends\",\"default-max-batch-size\":\"4\"}}\n",
      "I0626 22:19:23.253640 3220 python.cc:2296] Shared memory configuration is shm-default-byte-size=67108864,shm-growth-byte-size=67108864,stub-timeout-seconds=30\n",
      "I0626 22:19:23.253769 3220 python.cc:2344] TRITONBACKEND_ModelInitialize: postprocessing (version 1)\n",
      "I0626 22:19:23.254424 3220 python.cc:2388] TRITONBACKEND_ModelInstanceInitialize: postprocessing_0 (CPU device 0)\n",
      "I0626 22:19:23.254439 3220 backend_model_instance.cc:68] Creating instance postprocessing_0 on CPU using artifact ''\n",
      "I0626 22:19:23.261406 3299 python.cc:772] Starting Python backend stub:  exec /opt/tritonserver/backends/python/triton_python_backend_stub /workspace/triton-serve-fil/postprocessing/1/model.py triton_python_backend_shm_region_1 67108864 67108864 3220 /opt/tritonserver/backends/python 336 postprocessing_0\n",
      "I0626 22:19:23.344518 3220 backend_model.cc:292] Adding default backend config setting: default-max-batch-size,4\n",
      "I0626 22:19:23.353622 3220 python.cc:2409] TRITONBACKEND_ModelInstanceInitialize: instance initialization successful postprocessing_0 (device 0)\n",
      "I0626 22:19:23.353667 3220 python.cc:2344] TRITONBACKEND_ModelInitialize: preprocessing (version 1)\n",
      "I0626 22:19:23.353713 3220 backend_model_instance.cc:687] Starting backend thread for postprocessing_0 at nice 0 on device 0...\n",
      "I0626 22:19:23.353782 3220 model_repository_manager.cc:1345] successfully loaded 'postprocessing' version 1\n",
      "I0626 22:19:23.354163 3220 python.cc:2065] Using Python execution env /workspace/triton-serve-fil/preprocessing/rapids22.06_cuda11.5_py3.8.tar.gz\n",
      "I0626 22:19:23.354397 3220 python.cc:2388] TRITONBACKEND_ModelInstanceInitialize: preprocessing_0 (CPU device 0)\n",
      "I0626 22:19:23.354409 3220 backend_model_instance.cc:68] Creating instance preprocessing_0 on CPU using artifact ''\n",
      "I0626 22:19:51.590522 3376 python.cc:772] Starting Python backend stub: source /tmp/python_env_XrsHoI/0/bin/activate && exec env LD_LIBRARY_PATH=/tmp/python_env_XrsHoI/0/lib:$LD_LIBRARY_PATH /opt/tritonserver/backends/python/triton_python_backend_stub /workspace/triton-serve-fil/preprocessing/1/model.py triton_python_backend_shm_region_2 67108864 67108864 3220 /opt/tritonserver/backends/python 336 preprocessing_0\n",
      "I0626 22:19:52.231550 3220 python.cc:2409] TRITONBACKEND_ModelInstanceInitialize: instance initialization successful preprocessing_0 (device 0)\n",
      "I0626 22:19:52.231740 3220 backend_model_instance.cc:687] Starting backend thread for preprocessing_0 at nice 0 on device 0...\n",
      "I0626 22:19:52.231844 3220 model_repository_manager.cc:1345] successfully loaded 'preprocessing' version 1\n",
      "I0626 22:19:52.232071 3220 model_repository_manager.cc:1191] loading: ensemble_preprocess_fil_postprocess:1\n",
      "I0626 22:19:52.380127 3220 ensemble_model.cc:54] ensemble model for ensemble_preprocess_fil_postprocess\n",
      "\n",
      "I0626 22:19:52.380156 3220 model_repository_manager.cc:1345] successfully loaded 'ensemble_preprocess_fil_postprocess' version 1\n",
      "I0626 22:19:52.380245 3220 server.cc:556] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I0626 22:19:52.380311 3220 server.cc:583] \n",
      "+---------+---------------------------------+---------------------------------+\n",
      "| Backend | Path                            | Config                          |\n",
      "+---------+---------------------------------+---------------------------------+\n",
      "| fil     | /opt/tritonserver/backends/fil/ | {\"cmdline\":{\"auto-complete-conf |\n",
      "|         | libtriton_fil.so                | ig\":\"false\",\"min-compute-capabi |\n",
      "|         |                                 | lity\":\"6.000000\",\"backend-direc |\n",
      "|         |                                 | tory\":\"/opt/tritonserver/backen |\n",
      "|         |                                 | ds\",\"default-max-batch-size\":\"4 |\n",
      "|         |                                 | \"}}                             |\n",
      "|         |                                 |                                 |\n",
      "| python  | /opt/tritonserver/backends/pyth | {\"cmdline\":{\"auto-complete-conf |\n",
      "|         | on/libtriton_python.so          | ig\":\"false\",\"min-compute-capabi |\n",
      "|         |                                 | lity\":\"6.000000\",\"backend-direc |\n",
      "|         |                                 | tory\":\"/opt/tritonserver/backen |\n",
      "|         |                                 | ds\",\"default-max-batch-size\":\"4 |\n",
      "|         |                                 | \"}}                             |\n",
      "|         |                                 |                                 |\n",
      "+---------+---------------------------------+---------------------------------+\n",
      "\n",
      "I0626 22:19:52.380385 3220 server.cc:626] \n",
      "+-------------------------------------+---------+--------+\n",
      "| Model                               | Version | Status |\n",
      "+-------------------------------------+---------+--------+\n",
      "| ensemble_preprocess_fil_postprocess | 1       | READY  |\n",
      "| fil                                 | 1       | READY  |\n",
      "| postprocessing                      | 1       | READY  |\n",
      "| preprocessing                       | 1       | READY  |\n",
      "+-------------------------------------+---------+--------+\n",
      "\n",
      "I0626 22:19:52.410972 3220 metrics.cc:650] Collecting metrics for GPU 0: NVIDIA RTX A6000\n",
      "I0626 22:19:52.411218 3220 tritonserver.cc:2138] \n",
      "+----------------------------------+------------------------------------------+\n",
      "| Option                           | Value                                    |\n",
      "+----------------------------------+------------------------------------------+\n",
      "| server_id                        | triton                                   |\n",
      "| server_version                   | 2.22.0                                   |\n",
      "| server_extensions                | classification sequence model_repository |\n",
      "|                                  |  model_repository(unload_dependents) sch |\n",
      "|                                  | edule_policy model_configuration system_ |\n",
      "|                                  | shared_memory cuda_shared_memory binary_ |\n",
      "|                                  | tensor_data statistics trace             |\n",
      "| model_repository_path[0]         | /workspace/triton-serve-fil              |\n",
      "| model_control_mode               | MODE_NONE                                |\n",
      "| strict_model_config              | 1                                        |\n",
      "| rate_limit                       | OFF                                      |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                 |\n",
      "| response_cache_byte_size         | 0                                        |\n",
      "| min_supported_compute_capability | 6.0                                      |\n",
      "| strict_readiness                 | 1                                        |\n",
      "| exit_timeout                     | 30                                       |\n",
      "+----------------------------------+------------------------------------------+\n",
      "\n",
      "I0626 22:19:52.411276 3220 grpc_server.cc:4533] === GRPC KeepAlive Options ===\n",
      "I0626 22:19:52.411283 3220 grpc_server.cc:4534] keepalive_time_ms: 7200000\n",
      "I0626 22:19:52.411286 3220 grpc_server.cc:4536] keepalive_timeout_ms: 20000\n",
      "I0626 22:19:52.411288 3220 grpc_server.cc:4538] keepalive_permit_without_calls: 0\n",
      "I0626 22:19:52.411292 3220 grpc_server.cc:4540] http2_max_pings_without_data: 2\n",
      "I0626 22:19:52.411298 3220 grpc_server.cc:4542] http2_min_recv_ping_interval_without_data_ms: 300000\n",
      "I0626 22:19:52.411300 3220 grpc_server.cc:4545] http2_max_ping_strikes: 2\n",
      "I0626 22:19:52.411305 3220 grpc_server.cc:4547] ==============================\n",
      "I0626 22:19:52.411782 3220 grpc_server.cc:225] Ready for RPC 'ServerLive', 0\n",
      "I0626 22:19:52.411803 3220 grpc_server.cc:225] Ready for RPC 'ServerReady', 0\n",
      "I0626 22:19:52.411814 3220 grpc_server.cc:225] Ready for RPC 'ModelReady', 0\n",
      "I0626 22:19:52.411822 3220 grpc_server.cc:225] Ready for RPC 'ServerMetadata', 0\n",
      "I0626 22:19:52.411831 3220 grpc_server.cc:225] Ready for RPC 'ModelMetadata', 0\n",
      "I0626 22:19:52.411840 3220 grpc_server.cc:225] Ready for RPC 'ModelConfig', 0\n",
      "I0626 22:19:52.411848 3220 grpc_server.cc:225] Ready for RPC 'ModelStatistics', 0\n",
      "I0626 22:19:52.411857 3220 grpc_server.cc:225] Ready for RPC 'Trace', 0\n",
      "I0626 22:19:52.411866 3220 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryStatus', 0\n",
      "I0626 22:19:52.411875 3220 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryRegister', 0\n",
      "I0626 22:19:52.411884 3220 grpc_server.cc:225] Ready for RPC 'SystemSharedMemoryUnregister', 0\n",
      "I0626 22:19:52.411892 3220 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryStatus', 0\n",
      "I0626 22:19:52.411900 3220 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryRegister', 0\n",
      "I0626 22:19:52.411909 3220 grpc_server.cc:225] Ready for RPC 'CudaSharedMemoryUnregister', 0\n",
      "I0626 22:19:52.411917 3220 grpc_server.cc:225] Ready for RPC 'RepositoryIndex', 0\n",
      "I0626 22:19:52.411926 3220 grpc_server.cc:225] Ready for RPC 'RepositoryModelLoad', 0\n",
      "I0626 22:19:52.411936 3220 grpc_server.cc:225] Ready for RPC 'RepositoryModelUnload', 0\n",
      "I0626 22:19:52.411948 3220 grpc_server.cc:419] Thread started for CommonHandler\n",
      "I0626 22:19:52.412004 3220 grpc_server.cc:3587] New request handler for ModelInferHandler, 0\n",
      "I0626 22:19:52.412020 3220 grpc_server.cc:2511] Thread started for ModelInferHandler\n",
      "I0626 22:19:52.412063 3220 grpc_server.cc:3587] New request handler for ModelInferHandler, 0\n",
      "I0626 22:19:52.412077 3220 grpc_server.cc:2511] Thread started for ModelInferHandler\n",
      "I0626 22:19:52.412132 3220 grpc_server.cc:3971] New request handler for ModelStreamInferHandler, 0\n",
      "I0626 22:19:52.412146 3220 grpc_server.cc:2511] Thread started for ModelStreamInferHandler\n",
      "I0626 22:19:52.412152 3220 grpc_server.cc:4589] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I0626 22:19:52.412337 3220 http_server.cc:3303] Started HTTPService at 0.0.0.0:8000\n",
      "I0626 22:19:52.453211 3220 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002\n",
      "I0626 22:20:05.481949 3220 http_server.cc:3203] HTTP request: 2 /v2/models/ensemble_preprocess_fil_postprocess/infer\n",
      "I0626 22:20:05.482032 3220 infer_request.cc:710] prepared: [0x0x7f5a94003010] request id: , model: ensemble_preprocess_fil_postprocess, requested version: -1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 4, priority: 0, timeout (us): 0\n",
      "original inputs:\n",
      "[0x0x7f5a940136b8] input: raw_data, type: BYTES, original shape: [4,14], batch + shape: [4,14], shape: [14]\n",
      "override inputs:\n",
      "inputs:\n",
      "[0x0x7f5a940136b8] input: raw_data, type: BYTES, original shape: [4,14], batch + shape: [4,14], shape: [14]\n",
      "original requested outputs:\n",
      "requested outputs:\n",
      "processed_predictions\n",
      "\n",
      "I0626 22:20:05.482092 3220 infer_request.cc:710] prepared: [0x0x7f5a94004490] request id: , model: preprocessing, requested version: -1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 4, priority: 0, timeout (us): 0\n",
      "original inputs:\n",
      "[0x0x7f5a940047e8] input: INPUT, type: BYTES, original shape: [4,14], batch + shape: [4,14], shape: [14]\n",
      "override inputs:\n",
      "inputs:\n",
      "[0x0x7f5a940047e8] input: INPUT, type: BYTES, original shape: [4,14], batch + shape: [4,14], shape: [14]\n",
      "original requested outputs:\n",
      "OUTPUT\n",
      "requested outputs:\n",
      "OUTPUT\n",
      "\n",
      "I0626 22:20:05.482201 3220 python.cc:1553] model preprocessing, instance preprocessing_0, executing 1 requests\n",
      "I0626 22:20:05.551174 3220 infer_response.cc:167] add response output: output: OUTPUT, type: FP32, shape: [4,15]\n",
      "I0626 22:20:05.551221 3220 pinned_memory_manager.cc:161] pinned memory allocation: size 240, addr 0x7f5c50000090\n",
      "I0626 22:20:05.551231 3220 ensemble_scheduler.cc:540] Internal response allocation: OUTPUT, size 240, addr 0x7f5c50000090, memory type 1, type id 0\n",
      "I0626 22:20:05.551263 3220 ensemble_scheduler.cc:555] Internal response release: size 240, addr 0x7f5c50000090\n",
      "I0626 22:20:05.551287 3220 infer_request.cc:710] prepared: [0x0x7f5ac8004c20] request id: , model: fil, requested version: -1, actual version: 1, flags: 0x0, correlation id: 0, batch size: 4, priority: 0, timeout (us): 0\n",
      "original inputs:\n",
      "[0x0x7f5ac8004f58] input: input__0, type: FP32, original shape: [4,15], batch + shape: [4,15], shape: [15]\n",
      "override inputs:\n",
      "inputs:\n",
      "[0x0x7f5ac8004f58] input: input__0, type: FP32, original shape: [4,15], batch + shape: [4,15], shape: [15]\n",
      "original requested outputs:\n",
      "output__0\n",
      "requested outputs:\n",
      "output__0\n",
      "\n",
      "I0626 22:20:05.551341 3220 python.cc:2491] TRITONBACKEND_ModelInstanceExecute: model instance name preprocessing_0 released 1 requests\n",
      "I0626 22:20:05.551653 3220 infer_response.cc:167] add response output: output: output__0, type: FP32, shape: [4]\n",
      "I0626 22:20:05.551677 3220 ensemble_scheduler.cc:540] Internal response allocation: output__0, size 16, addr 0x7f5c4c00f200, memory type 2, type id 0\n",
      "I0626 22:20:05.551712 3220 ensemble_scheduler.cc:555] Internal response release: size 16, addr 0x7f5c4c00f200\n",
      "I0626 22:20:05.551765 3220 pinned_memory_manager.cc:190] pinned memory deallocation: addr 0x7f5c50000090\n",
      "^C\n",
      "Signal (2) received.\n",
      "I0626 22:24:04.667263 3220 server.cc:257] Waiting for in-flight requests to complete.\n",
      "I0626 22:24:04.667286 3220 server.cc:273] Timeout 30: Found 0 model versions that have in-flight inferences\n",
      "I0626 22:24:04.667296 3220 model_repository_manager.cc:1223] unloading: postprocessing:1\n",
      "I0626 22:24:04.667380 3220 model_repository_manager.cc:1223] unloading: fil:1\n",
      "I0626 22:24:04.667418 3220 model_repository_manager.cc:1223] unloading: preprocessing:1\n",
      "I0626 22:24:04.667518 3220 model_repository_manager.cc:1223] unloading: ensemble_preprocess_fil_postprocess:1\n",
      "I0626 22:24:04.667555 3220 backend_model_instance.cc:710] Stopping backend thread for fil_0...\n",
      "I0626 22:24:04.667616 3220 backend_model_instance.cc:710] Stopping backend thread for postprocessing_0...\n",
      "I0626 22:24:04.667635 3220 backend_model_instance.cc:710] Stopping backend thread for preprocessing_0...\n",
      "I0626 22:24:04.667664 3220 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0626 22:24:04.667672 3220 server.cc:288] All models are stopped, unloading models\n",
      "I0626 22:24:04.667699 3220 python.cc:2509] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0626 22:24:04.667706 3220 server.cc:295] Timeout 30: Found 4 live models and 0 in-flight non-inference requests\n",
      "I0626 22:24:04.667687 3220 python.cc:2509] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0626 22:24:04.667734 3220 server.cc:302] ensemble_preprocess_fil_postprocess v1: UNLOADING\n",
      "I0626 22:24:04.667764 3220 server.cc:302] fil v1: UNLOADING\n",
      "I0626 22:24:04.667775 3220 server.cc:302] postprocessing v1: UNLOADING\n",
      "I0626 22:24:04.667785 3220 server.cc:302] preprocessing v1: UNLOADING\n",
      "I0626 22:24:04.667928 3220 model_repository_manager.cc:1328] successfully unloaded 'ensemble_preprocess_fil_postprocess' version 1\n",
      "I0626 22:24:04.668060 3220 model_finalize.hpp:36] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0626 22:24:04.668125 3220 dynamic_batch_scheduler.cc:423] Stopping dynamic-batcher thread for \\\u0003d...\n",
      "I0626 22:24:04.668201 3220 model_repository_manager.cc:1328] successfully unloaded 'fil' version 1\n"
     ]
    }
   ],
   "source": [
    "!tritonserver --model-repository `pwd`/triton-serve-fil --log-verbose 1 --exit-on-error=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96dbb7c-6549-46a1-9e8c-c818963e6e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
