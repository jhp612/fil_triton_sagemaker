{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "770966e6-798c-4b64-93c2-cf14d58dfca9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0627 18:32:28.484922 1428 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7f4b04000000' with size 268435456\n",
      "I0627 18:32:28.485211 1428 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "W0627 18:32:28.486984 1428 model_repository_manager.cc:315] ignore version directory '.ipynb_checkpoints' which fails to convert to integral number\n",
      "I0627 18:32:28.487008 1428 model_repository_manager.cc:1191] loading: fil:1\n",
      "I0627 18:32:28.587258 1428 model_repository_manager.cc:1191] loading: preprocessing:1\n",
      "I0627 18:32:28.600016 1428 initialize.hpp:43] TRITONBACKEND_Initialize: fil\n",
      "I0627 18:32:28.600051 1428 backend.hpp:47] Triton TRITONBACKEND API version: 1.9\n",
      "I0627 18:32:28.600066 1428 backend.hpp:52] 'fil' TRITONBACKEND API version: 1.9\n",
      "I0627 18:32:28.600585 1428 model_initialize.hpp:37] TRITONBACKEND_ModelInitialize: fil (version 1)\n",
      "I0627 18:32:28.602484 1428 instance_initialize.hpp:46] TRITONBACKEND_ModelInstanceInitialize: fil_0 (GPU device 0)\n",
      "I0627 18:32:28.654640 1428 model_repository_manager.cc:1345] successfully loaded 'fil' version 1\n",
      "W0627 18:32:28.687523 1428 model_repository_manager.cc:315] ignore version directory '.ipynb_checkpoints' which fails to convert to integral number\n",
      "I0627 18:32:28.687564 1428 model_repository_manager.cc:1191] loading: postprocessing:1\n",
      "I0627 18:32:28.689678 1428 python.cc:2065] Using Python execution env /workspace/triton-serve-fil/preprocessing/rapids22.06_cuda11.5_py3.8.tar.gz\n",
      "I0627 18:32:28.689903 1428 python.cc:2388] TRITONBACKEND_ModelInstanceInitialize: preprocessing_0 (CPU device 0)\n",
      "I0627 18:32:57.993829 1428 model_repository_manager.cc:1345] successfully loaded 'preprocessing' version 1\n",
      "I0627 18:32:57.994412 1428 python.cc:2388] TRITONBACKEND_ModelInstanceInitialize: postprocessing_0 (CPU device 0)\n",
      "I0627 18:32:58.101947 1428 model_repository_manager.cc:1345] successfully loaded 'postprocessing' version 1\n",
      "W0627 18:32:58.102253 1428 model_repository_manager.cc:315] ignore version directory '.ipynb_checkpoints' which fails to convert to integral number\n",
      "I0627 18:32:58.102302 1428 model_repository_manager.cc:1191] loading: ensemble_preprocess_fil_postprocess:1\n",
      "I0627 18:32:58.247568 1428 model_repository_manager.cc:1345] successfully loaded 'ensemble_preprocess_fil_postprocess' version 1\n",
      "I0627 18:32:58.247652 1428 server.cc:556] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I0627 18:32:58.247719 1428 server.cc:583] \n",
      "+---------+---------------------------------+---------------------------------+\n",
      "| Backend | Path                            | Config                          |\n",
      "+---------+---------------------------------+---------------------------------+\n",
      "| fil     | /opt/tritonserver/backends/fil/ | {\"cmdline\":{\"auto-complete-conf |\n",
      "|         | libtriton_fil.so                | ig\":\"false\",\"min-compute-capabi |\n",
      "|         |                                 | lity\":\"6.000000\",\"backend-direc |\n",
      "|         |                                 | tory\":\"/opt/tritonserver/backen |\n",
      "|         |                                 | ds\",\"default-max-batch-size\":\"4 |\n",
      "|         |                                 | \"}}                             |\n",
      "|         |                                 |                                 |\n",
      "| python  | /opt/tritonserver/backends/pyth | {\"cmdline\":{\"auto-complete-conf |\n",
      "|         | on/libtriton_python.so          | ig\":\"false\",\"min-compute-capabi |\n",
      "|         |                                 | lity\":\"6.000000\",\"backend-direc |\n",
      "|         |                                 | tory\":\"/opt/tritonserver/backen |\n",
      "|         |                                 | ds\",\"default-max-batch-size\":\"4 |\n",
      "|         |                                 | \"}}                             |\n",
      "|         |                                 |                                 |\n",
      "+---------+---------------------------------+---------------------------------+\n",
      "\n",
      "I0627 18:32:58.247786 1428 server.cc:626] \n",
      "+-------------------------------------+---------+--------+\n",
      "| Model                               | Version | Status |\n",
      "+-------------------------------------+---------+--------+\n",
      "| ensemble_preprocess_fil_postprocess | 1       | READY  |\n",
      "| fil                                 | 1       | READY  |\n",
      "| postprocessing                      | 1       | READY  |\n",
      "| preprocessing                       | 1       | READY  |\n",
      "+-------------------------------------+---------+--------+\n",
      "\n",
      "I0627 18:32:58.278722 1428 metrics.cc:650] Collecting metrics for GPU 0: NVIDIA RTX A6000\n",
      "I0627 18:32:58.279187 1428 tritonserver.cc:2138] \n",
      "+----------------------------------+------------------------------------------+\n",
      "| Option                           | Value                                    |\n",
      "+----------------------------------+------------------------------------------+\n",
      "| server_id                        | triton                                   |\n",
      "| server_version                   | 2.22.0                                   |\n",
      "| server_extensions                | classification sequence model_repository |\n",
      "|                                  |  model_repository(unload_dependents) sch |\n",
      "|                                  | edule_policy model_configuration system_ |\n",
      "|                                  | shared_memory cuda_shared_memory binary_ |\n",
      "|                                  | tensor_data statistics trace             |\n",
      "| model_repository_path[0]         | /workspace/triton-serve-fil              |\n",
      "| model_control_mode               | MODE_NONE                                |\n",
      "| strict_model_config              | 1                                        |\n",
      "| rate_limit                       | OFF                                      |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                 |\n",
      "| response_cache_byte_size         | 0                                        |\n",
      "| min_supported_compute_capability | 6.0                                      |\n",
      "| strict_readiness                 | 1                                        |\n",
      "| exit_timeout                     | 30                                       |\n",
      "+----------------------------------+------------------------------------------+\n",
      "\n",
      "I0627 18:32:58.280119 1428 grpc_server.cc:4589] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I0627 18:32:58.280335 1428 http_server.cc:3303] Started HTTPService at 0.0.0.0:8000\n",
      "I0627 18:32:58.321390 1428 http_server.cc:178] Started Metrics Service at 0.0.0.0:8002\n",
      "^C\n",
      "Signal (2) received.\n",
      "I0627 20:53:55.729586 1428 server.cc:257] Waiting for in-flight requests to complete.\n",
      "I0627 20:53:55.729606 1428 server.cc:273] Timeout 30: Found 0 model versions that have in-flight inferences\n",
      "I0627 20:53:55.729616 1428 model_repository_manager.cc:1223] unloading: postprocessing:1\n",
      "I0627 20:53:55.729690 1428 model_repository_manager.cc:1223] unloading: fil:1\n",
      "I0627 20:53:55.729758 1428 model_repository_manager.cc:1223] unloading: preprocessing:1\n",
      "I0627 20:53:55.729818 1428 model_repository_manager.cc:1223] unloading: ensemble_preprocess_fil_postprocess:1\n",
      "I0627 20:53:55.729853 1428 server.cc:288] All models are stopped, unloading models\n",
      "I0627 20:53:55.729871 1428 server.cc:295] Timeout 30: Found 4 live models and 0 in-flight non-inference requests\n",
      "I0627 20:53:55.729884 1428 instance_finalize.hpp:36] TRITONBACKEND_ModelInstanceFinalize: delete instance state\n",
      "I0627 20:53:55.729937 1428 model_repository_manager.cc:1328] successfully unloaded 'ensemble_preprocess_fil_postprocess' version 1\n",
      "I0627 20:53:55.730213 1428 model_finalize.hpp:36] TRITONBACKEND_ModelFinalize: delete model state\n",
      "I0627 20:53:55.730268 1428 model_repository_manager.cc:1328] successfully unloaded 'fil' version 1\n"
     ]
    }
   ],
   "source": [
    "!tritonserver --model-repository `pwd`/triton-serve-fil --log-verbose 0 --exit-on-error=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96dbb7c-6549-46a1-9e8c-c818963e6e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
