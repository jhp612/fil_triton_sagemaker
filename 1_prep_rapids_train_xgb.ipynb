{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "300f2516",
   "metadata": {},
   "source": [
    "# Data Preprocessing using RAPIDS and Training XGBoost for Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c380bf",
   "metadata": {},
   "source": [
    "In this notebook we will walk through using [RAPIDS](https://rapids.ai/about.html) for GPU-accelerated data preprocessing and training of XGBoost model for Fraud Detection use-case. In the [second notebook](2_triton_xgb_fil_ensemble.ipynb) we will show how to deploy the trained XGBoost model in Triton on SageMaker. The RAPIDS suite of open source software libraries and APIs gives you the ability to execute end-to-end data science and analytics pipelines entirely on GPUs.\n",
    "\n",
    "**Note:** Since the primary goal of this example is to get a trained XGBoost model to illustrate deployment of Tree-based ML models on Triton in SageMaker we don't perform any in-depth feature engineering or hyperparameter optimization. Although RAPIDS on SageMaker is excellent for [running cost-effective HPO in minimal amount of time](https://aws.amazon.com/blogs/machine-learning/rapids-and-amazon-sagemaker-scale-up-and-scale-out-to-tackle-ml-challenges/) to get to the best accuracy model configuration. \n",
    "\n",
    "## To Run This Notebook Please Select RAPIDS 2106 Kernel from the Kernel Dropdown menu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa02b6-b529-403f-a77d-1d28f87f6159",
   "metadata": {},
   "source": [
    "This notebook was tested with the `rapids2106` kernel on an Amazon SageMaker notebook instance of type `g4dn`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc15a78",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff93c3b",
   "metadata": {},
   "source": [
    "For this example, we use the Tabformer [synthetic credit card transactions dataset](https://arxiv.org/abs/1910.03033) from IBM available on [Kaggle](https://www.kaggle.com/datasets/ealtman2019/credit-card-transactions). You can either directly download the dataset from this [Kaggle link](https://www.kaggle.com/datasets/ealtman2019/credit-card-transactions) and then upload it to your SageMaker notebook instance. Or you may fetch the data from Kaggle command line client using the following commands.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fbb403",
   "metadata": {},
   "source": [
    "### Set up Kaggle API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f4384e",
   "metadata": {},
   "source": [
    "First we install the Kaggle CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81de5c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8394de80",
   "metadata": {},
   "source": [
    "Then we enable the Kaggle API. This assumes you have an account on Kaggle. It's free and only takes a minute. Once you have that, follow [instructions here](https://github.com/Kaggle/kaggle-api#api-credentials) to retrieve your kaggle.json file and upload it to SageMaker through JupyterLab upload interface. Then run the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d35177a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /home/ec2-user/.kaggle\n",
    "!mv kaggle.json /home/ec2-user/.kaggle/\n",
    "!chmod 600 /home/ec2-user/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b962aaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading credit-card-transactions.zip to /home/ec2-user/SageMaker/fil_triton_sagemaker\n",
      "100%|███████████████████████████████████████▉| 263M/263M [00:57<00:00, 4.94MB/s]\n",
      "100%|████████████████████████████████████████| 263M/263M [00:57<00:00, 4.79MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d ealtman2019/credit-card-transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be3cd88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  credit-card-transactions.zip\n",
      "  inflating: User0_credit_card_transactions.csv  \n",
      "  inflating: credit_card_transactions-ibm_v2.csv  \n",
      "  inflating: sd254_cards.csv         \n",
      "  inflating: sd254_users.csv         \n"
     ]
    }
   ],
   "source": [
    "!unzip -u credit-card-transactions.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a090f1-9a9e-4d20-b018-46f7551855fc",
   "metadata": {},
   "source": [
    "## Check on our GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2332ef",
   "metadata": {},
   "source": [
    "Next, let's check the GPU resources we have by using the terminal command `nvidia-smi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d45894ea-2ed7-4d5f-9775-7452a25054b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 19 01:23:42 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:41:00.0  On |                  Off |\n",
      "| 31%   54C    P8    34W / 300W |   8105MiB / 49140MiB |     30%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "GPU 0: NVIDIA RTX A6000 (UUID: GPU-a47b8f50-700f-1098-205a-b1ae71e50aca)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea35974f-2be4-4db8-82f6-da435e993fb5",
   "metadata": {},
   "source": [
    "Awesome, we have powerful NVIDIA GPU at our disposal. Let's get started with using it for Data Preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0948fb8-78e2-4ada-9805-c726a6d35106",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ba8545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cuml\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe135dc-dead-4268-9ac0-1a9aa909f087",
   "metadata": {},
   "source": [
    "We read in the data and begin our data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "050ea3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Card</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Time</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Use Chip</th>\n",
       "      <th>Merchant Name</th>\n",
       "      <th>Merchant City</th>\n",
       "      <th>Merchant State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>MCC</th>\n",
       "      <th>Errors?</th>\n",
       "      <th>Is Fraud?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>06:21</td>\n",
       "      <td>$134.09</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>3527213246127876953</td>\n",
       "      <td>La Verne</td>\n",
       "      <td>CA</td>\n",
       "      <td>91750.0</td>\n",
       "      <td>5300</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>06:42</td>\n",
       "      <td>$38.48</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>-727612092139916043</td>\n",
       "      <td>Monterey Park</td>\n",
       "      <td>CA</td>\n",
       "      <td>91754.0</td>\n",
       "      <td>5411</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>06:22</td>\n",
       "      <td>$120.34</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>-727612092139916043</td>\n",
       "      <td>Monterey Park</td>\n",
       "      <td>CA</td>\n",
       "      <td>91754.0</td>\n",
       "      <td>5411</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>17:45</td>\n",
       "      <td>$128.95</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>3414527459579106770</td>\n",
       "      <td>Monterey Park</td>\n",
       "      <td>CA</td>\n",
       "      <td>91754.0</td>\n",
       "      <td>5651</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>06:23</td>\n",
       "      <td>$104.71</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>5817218446178736267</td>\n",
       "      <td>La Verne</td>\n",
       "      <td>CA</td>\n",
       "      <td>91750.0</td>\n",
       "      <td>5912</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User  Card  Year  Month  Day   Time   Amount           Use Chip  \\\n",
       "0     0     0  2002      9    1  06:21  $134.09  Swipe Transaction   \n",
       "1     0     0  2002      9    1  06:42   $38.48  Swipe Transaction   \n",
       "2     0     0  2002      9    2  06:22  $120.34  Swipe Transaction   \n",
       "3     0     0  2002      9    2  17:45  $128.95  Swipe Transaction   \n",
       "4     0     0  2002      9    3  06:23  $104.71  Swipe Transaction   \n",
       "\n",
       "         Merchant Name  Merchant City Merchant State      Zip   MCC Errors?  \\\n",
       "0  3527213246127876953       La Verne             CA  91750.0  5300    <NA>   \n",
       "1  -727612092139916043  Monterey Park             CA  91754.0  5411    <NA>   \n",
       "2  -727612092139916043  Monterey Park             CA  91754.0  5411    <NA>   \n",
       "3  3414527459579106770  Monterey Park             CA  91754.0  5651    <NA>   \n",
       "4  5817218446178736267       La Verne             CA  91750.0  5912    <NA>   \n",
       "\n",
       "  Is Fraud?  \n",
       "0        No  \n",
       "1        No  \n",
       "2        No  \n",
       "3        No  \n",
       "4        No  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = './'\n",
    "data_csv = 'credit_card_transactions-ibm_v2.csv'\n",
    "data = cudf.read_csv(os.path.join(data_path, data_csv))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f509ed30-77d4-4ca1-8f13-a571011bf4a7",
   "metadata": {},
   "source": [
    "Each row here is a credit card transaction with its attributes like time and amount of transaction along with merchant attributes like Name, City, State, Zipcode and Merchant Category Code (MCC) and finally whether the transaction was fraudulent or legitimate (`Is Fraud?`). \n",
    "\n",
    "**Note:** `Merchant Name` is hashed so that's we see integers instead of strings.\n",
    "\n",
    "The original dataset has about 24 million rows but in this example we use subset of about ~5 million transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "644802ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4877380, 15)\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "data = data.sample(frac=0.2, random_state=SEED)\n",
    "data = data.reset_index(drop=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8234808-7c5a-405f-b1bc-5797ce8c834b",
   "metadata": {},
   "source": [
    "We convert some categorical features to dtype object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da4f2cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Zip'] = data['Zip'].astype('object')\n",
    "data['MCC'] = data['MCC'].astype('object')\n",
    "data[\"Merchant Name\"] = data[\"Merchant Name\"].astype(\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e769508",
   "metadata": {},
   "source": [
    "### Encode labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b52bff-cebd-4e1e-881f-489d27f49add",
   "metadata": {},
   "source": [
    "Next we perform encoding on our binary labels `Is Fraud?` which indicate whether a transaction is fraudulent or not. After encoding `1` will denote fraud and `0` will denote legitimate transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1a02ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Is Fraud?']\n",
    "data.drop(columns=['Is Fraud?'], inplace=True)\n",
    "y = (y == \"Yes\").astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728239b2",
   "metadata": {},
   "source": [
    "### Save subset for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4021fe51",
   "metadata": {},
   "source": [
    "We will also save a small subset of the data to submit Triton inference requests for later on in the [second notebook](2_triton_xgb_fil_ensemble.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a0b921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_infer = data.iloc[510:516]\n",
    "data_infer.to_csv('data_infer.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234c652b",
   "metadata": {},
   "source": [
    "### Handle Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6d5c43-5388-4394-817f-aff770aa3ff6",
   "metadata": {},
   "source": [
    "Next let's handle the missing values in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea224054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "User               0.000000\n",
       "Card               0.000000\n",
       "Year               0.000000\n",
       "Month              0.000000\n",
       "Day                0.000000\n",
       "Time               0.000000\n",
       "Amount             0.000000\n",
       "Use Chip           0.000000\n",
       "Merchant Name      0.000000\n",
       "Merchant City      0.000000\n",
       "Merchant State    11.143544\n",
       "Zip               11.791884\n",
       "MCC                0.000000\n",
       "Errors?           98.402626\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()/len(data) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e189a9-5d88-46a0-a714-ca57af719bec",
   "metadata": {},
   "source": [
    "We have some missing values in `Merchant State` and `Zip` columns. Turns out these correspond to Online transactions so we will set those missing values to `ONLINE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cb24124",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[\"Merchant City\"]==\"ONLINE\", \"Merchant State\"] = \"ONLINE\" \n",
    "data.loc[data[\"Merchant City\"]==\"ONLINE\", \"Zip\"] = \"ONLINE\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a18416-1547-4381-a8e9-6ff2554cd8cd",
   "metadata": {},
   "source": [
    "We also have some foreign transactions where `Merchant City` and `Merchant State` is a foreign city and country and the Zipcode is missing. For those transactions we will set the Zipcode to `FOREIGN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b103be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_states_plus_online = ['AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA',\n",
    "           'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME',\n",
    "           'MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM',\n",
    "           'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX',\n",
    "           'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY', 'ONLINE']\n",
    "\n",
    "# set zip of all transactions that are not in US States or Online to Foreign\n",
    "data.loc[~data[\"Merchant State\"].isin(us_states_plus_online), \"Zip\"] = \"FOREIGN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0620808b-4fdf-4afc-8c7c-94a800d6f752",
   "metadata": {},
   "source": [
    "The `Errors?` column indicates whether or not the transaction had any errors like Incorrect Pin associated with it. We make this a boolean indicator feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a3efa69-b747-43a5-82a4-5407d0c1998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Errors?'] = data['Errors?'].notna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5559c16e",
   "metadata": {},
   "source": [
    "### Handle Amount and Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c62654-6922-434b-ba1d-f3d2f9e01699",
   "metadata": {},
   "source": [
    "Next, for the `Amount` column we remove the dollar symbol prefix and for `Time` column we extract out the Hour and Minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88f70ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Amount'] = data['Amount'].str.slice(1)\n",
    "data['Hour'] = data['Time'].str.slice(stop=2)\n",
    "data['Minute'] = data['Time'].str.slice(start=3)\n",
    "data.drop(columns=['Time'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d96c68",
   "metadata": {},
   "source": [
    "###  Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596ac2f4-5f79-4cee-9f0e-b5b2a2a2e1f7",
   "metadata": {},
   "source": [
    "Before doing any further preprocessing let's perform the train-test split. Here we use 70-30 train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24f2137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.3, random_state=SEED, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43e63638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "383"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Free up some room on the GPU by explicitly deleting dataframes\n",
    "import gc\n",
    "del data\n",
    "del y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67200756",
   "metadata": {},
   "source": [
    "### Encoding Categorical Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7106b0c5-f74b-4e5e-ae89-855b57527b5b",
   "metadata": {},
   "source": [
    "Next, we handle categorical columns in our dataset by performing label encoding on them. For some of these columns we have some unseen values which are present in test data but not train data. We handle those values by setting them to `UNKNOWN` before doing the label encoding so that at test time we have an encoding for these unseen values.\n",
    "\n",
    "We also save the encodings for each categorical column so that we can later use them for doing data preprocessing at inference time in the [second notebook](2_triton_xgb_fil_ensemble.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67c8af93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.preprocessing import LabelEncoder\n",
    "categorial_columns = ['Zip', 'MCC', 'Merchant Name', 'Use Chip', 'Merchant City', 'Merchant State']\n",
    "encoders = {}\n",
    "\n",
    "# handle unknown values present in training data but not in test data\n",
    "for col in categorial_columns:\n",
    "    # convert cudf series to numpy array with .values_host\n",
    "    unique_values = X_train[col].unique().values_host\n",
    "    X_test.loc[~X_test[col].isin(unique_values), col] = 'UNKNOWN'\n",
    "    unique_values = np.append(unique_values, ['UNKNOWN'])\n",
    "    # convert numpy array to cudf series\n",
    "    unique_values = cudf.Series(unique_values)\n",
    "    le = LabelEncoder().fit(unique_values)\n",
    "    X_train[col] = le.transform(X_train[col])\n",
    "    X_test[col] = le.transform(X_test[col])\n",
    "    encoders[col] = le.classes_.values_host\n",
    "\n",
    "with open('label_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(encoders, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30f39b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all dtypes to fp32 for xgboost training\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1945a48f",
   "metadata": {},
   "source": [
    "## Train XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b30152",
   "metadata": {},
   "source": [
    "Now we train the XGBoost fraud detection model on our GPU. This will take about 2-3 minutes on `g4dn.xlarge` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eee1e1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import time\n",
    "\n",
    "dtrain = xgb.DMatrix(\n",
    "        X_train,\n",
    "        y_train)\n",
    "\n",
    "dtest = xgb.DMatrix(\n",
    "        X_test,\n",
    "        y_test)\n",
    "\n",
    "max_depth = 8\n",
    "num_trees = 2000\n",
    "xgb_params = {\n",
    "    'max_depth':          max_depth,\n",
    "    'tree_method':       'gpu_hist',\n",
    "    'objective':         'binary:logistic',\n",
    "    'eval_metric':       'aucpr',\n",
    "    'predictor':         'gpu_predictor',\n",
    "}\n",
    "model = xgb.train(params=xgb_params, \n",
    "                       dtrain=dtrain, \n",
    "                       num_boost_round=num_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a469c5d6-ad2d-47eb-98c0-4de98b1b56f6",
   "metadata": {},
   "source": [
    "We quickly evaluate our trained model's predictions on the test set using F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b2d1576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1-Score:  0.8344\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_score = model.predict(dtest)\n",
    "threshold = 0.5\n",
    "y_pred = (y_score >= 0.5).astype(int)\n",
    "y_true = y_test.values_host\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(f'Test F1-Score: {f1: 0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10be241e-86de-4a3c-927c-f9270d4771c2",
   "metadata": {},
   "source": [
    "We can do further Hyperparameter tuning/Feature Engineering to improve the model accuracy but since the primary goal of this example is to walkthrough deployment of decision tree-based ML models like XGBoost on Triton in SageMaker we save our trained model and move on to the [second notebook](2_triton_xgb_fil_ensemble.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c966c5cd",
   "metadata": {},
   "source": [
    "### Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e7b2291",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./xgboost.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ab5f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93410c13-1625-4c68-bd76-37a0f850977b",
   "metadata": {},
   "source": [
    "## Next Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d4c8dd-227c-4e60-aa92-d63c11473525",
   "metadata": {},
   "source": [
    "Please open the [second notebook](2_triton_xgb_fil_ensemble.ipynb) to learn how to deploy this XGBoost model and other similar decision tree-based ML models on Triton in SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d0fe3f-97b6-4f7c-b92f-cc9934bfdd60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
