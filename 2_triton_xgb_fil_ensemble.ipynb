{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b15ac3a8",
   "metadata": {},
   "source": [
    "# Pre-processing + XGBoost model inference pipeline with NVIDIA Triton Inference Server on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553e6cc5-2a63-45f9-80fb-2fcc65e38e86",
   "metadata": {},
   "source": [
    "With 22.05 release of [NVIDIA Triton](https://github.com/triton-inference-server/server/) container image on SageMaker you can now use Triton's [Forest Inference Library (FIL) backend](https://github.com/triton-inference-server/fil_backend) to easily serve tree based ML models like XGBoost for high-performance CPU and GPU inference in SageMaker. Using Triton's FIL backend allows you to benefit from the performance optimizations like dynamic batching, concurrent execution which help maximize the utilization of GPU and CPU, further lowering the cost of inference. And the multi-framework support provided by NVIDIA Triton allows you seamlessly deploy tree based ML models alongside deep learning models for fast, unified inference pipelines.\n",
    "\n",
    "Machine Learning applications are complex and can often require data pre-processing. So in this notebook, we will not only deep dive into how to deploy a tree-based ML model like XGBoost using the FIL Backend in Triton on SageMaker endpoint but we will also cover how to implement python-based data pre-processing inference pipeline for your model using the ensemble feature in Triton. This will allow us to send in the raw data from client side and have both data pre-processing and model inference happen in Triton SageMaker endpoint for the best inference performance.\n",
    "\n",
    "**Note:** This notebook was tested with the `conda_python3` kernel on an Amazon SageMaker notebook instance of type `g4dn`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8117224d-8b7c-446f-90b3-669def64cf71",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f2d18d-93b6-4b34-9776-689f0bf5eddb",
   "metadata": {},
   "source": [
    "## Forest Inference Library (FIL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb668d-5b88-427e-844d-6499f5cc98af",
   "metadata": {},
   "source": [
    "RAPIDS Forest Inference Library (FIL) is a library to provide high-performance inference for tree-based models. Here are some important FIL features:\n",
    "\n",
    "* Supports XGBoost, LightGBM, cuML RandomForest, and Scikit Learn Random Forest\n",
    "* No conversion needed for XGBoost and LightGBM. SKLearn or cuML pickle models need to be converted to Treelite's binary checkpoint format \n",
    "* SKLearn Random Forest is supported for single-output regression and multi-class classification\n",
    "* Both CPU and GPU are supported\n",
    "\n",
    "Below we show benchmark highlighting FIL's throughput performance against CPU XGBoost.\n",
    "\n",
    "<img src=\"./images/fil_benchmark.png\" alt=\"fil-benchmark\" width=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971c535f-2f2c-4c35-b4ad-8238503262ca",
   "metadata": {},
   "source": [
    "## Triton FIL Backend\n",
    "FIL is available as a backend in Triton with all of its features to allow for serving XGBoost, LightGBM and RandomForest models both on CPU and GPU with high performance. Here are some important features of the FIL Backend:\n",
    "\n",
    "* **Shapley Value Support (GPU)**: GPU Shapley Values are supported for Model Explainability\n",
    "* **Categorical Feature Support**: Models trained on categorical features fully supported.\n",
    "* **CPU Optimizations**: Optimized CPU mode offers faster execution than native XGBoost.\n",
    "\n",
    "To learn more about FIL Backend's features please see the [FAQ Notebook](https://github.com/triton-inference-server/fil_backend/blob/fea-faq_nb/notebooks/faq/FAQs.ipynb) and [Triton FIL Backend GitHub](https://github.com/triton-inference-server/fil_backend/tree/main)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fbd827-edf2-46af-8412-0be9d612f378",
   "metadata": {},
   "source": [
    "## Triton Model Ensemble Feature\n",
    "Triton Inference Server greatly simplifies the deployment of AI models at scale in production. Triton Server comes with a convenient solution that simplifies building pre-processing and post-processing pipelines. Triton Server platform provides the ensemble scheduler, which is responsible for pipelining models participating in the inference process while ensuring efficiency and optimizing throughput.\n",
    "\n",
    "<img src=\"./images/triton-ensemble.png\" alt=\"triton-ensemble\" width=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd4fa0e",
   "metadata": {},
   "source": [
    "## Set up Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6de40f",
   "metadata": {},
   "source": [
    "Installs the dependencies required to package the model and run inferences using Triton server.\n",
    "\n",
    "Also define the IAM role that will give SageMaker access to the model artifacts and the NVIDIA Triton ECR image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57035a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.0.1 requires botocore<1.22.9,>=1.22.8, but you have botocore 1.27.28 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting nvidia-pyindex\n",
      "  Downloading nvidia-pyindex-1.0.9.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nvidia-pyindex\n",
      "  Building wheel for nvidia-pyindex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-pyindex: filename=nvidia_pyindex-1.0.9-py3-none-any.whl size=8413 sha256=077d30080ee065966b189126fe527b082995e97f998d94e654e8a27ca5a5ef3c\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/e0/c2/fb/5cf4e1cfaf28007238362cb746fb38fc2dd76348331a748d54\n",
      "Successfully built nvidia-pyindex\n",
      "Installing collected packages: nvidia-pyindex\n",
      "Successfully installed nvidia-pyindex-1.0.9\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com, https://pypi.ngc.nvidia.com\n",
      "Collecting tritonclient[http]\n",
      "  Downloading tritonclient-2.23.0-py3-none-manylinux1_x86_64.whl (8.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting python-rapidjson>=0.9.1\n",
      "  Downloading python_rapidjson-1.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m346.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from tritonclient[http]) (1.20.3)\n",
      "Collecting geventhttpclient>=1.4.4\n",
      "  Downloading geventhttpclient-1.5.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m281.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from geventhttpclient>=1.4.4->tritonclient[http]) (1.16.0)\n",
      "Collecting brotli\n",
      "  Downloading Brotli-1.0.9-cp38-cp38-manylinux1_x86_64.whl (357 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.2/357.2 kB\u001b[0m \u001b[31m347.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from geventhttpclient>=1.4.4->tritonclient[http]) (2021.10.8)\n",
      "Requirement already satisfied: gevent>=0.13 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from geventhttpclient>=1.4.4->tritonclient[http]) (21.8.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[http]) (59.4.0)\n",
      "Requirement already satisfied: zope.event in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[http]) (4.5.0)\n",
      "Requirement already satisfied: zope.interface in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[http]) (5.4.0)\n",
      "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[http]) (1.1.2)\n",
      "Installing collected packages: brotli, python-rapidjson, tritonclient, geventhttpclient\n",
      "Successfully installed brotli-1.0.9 geventhttpclient-1.5.5 python-rapidjson-1.8 tritonclient-2.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU pip awscli boto3 sagemaker \n",
    "!pip install nvidia-pyindex\n",
    "!pip install tritonclient[http]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2017a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "import time\n",
    "import os\n",
    "from sagemaker import get_execution_role\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "796628a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd33ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise(\"UNSUPPORTED REGION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "962f0513",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "triton_image_uri = \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.05-py3\".format(\n",
    "    account_id=account_id_map[region], region=region, base=base\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec15697b",
   "metadata": {},
   "source": [
    "## Package models and dependencies and uploading to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d78bf40-415d-4fd8-86ad-bea9c9a10b78",
   "metadata": {},
   "source": [
    "The following example shows the model repository directory structure, containing a DALI preprocessing model, TensorFlow Inception v3 model, and the model ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a51d2a",
   "metadata": {},
   "source": [
    "### Create Config File for FIL Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc64cb6",
   "metadata": {},
   "source": [
    "First we create the Triton config file for the XGBoost model being served by the FIL Backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b61b7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "FIL_MODEL_DIR = './model_repository/fil'\n",
    "\n",
    "# Maximum size in bytes for input and output arrays\n",
    "MAX_MEMORY_BYTES = 60_000_000\n",
    "NUM_FEATURES = 15\n",
    "NUM_CLASSES = 2\n",
    "bytes_per_sample = (NUM_FEATURES + NUM_CLASSES) * 4\n",
    "max_batch_size = MAX_MEMORY_BYTES // bytes_per_sample\n",
    "\n",
    "IS_CLASSIFIER = True\n",
    "model_format = 'xgboost_json'\n",
    "\n",
    "# Select deployment hardware (GPU or CPU)\n",
    "if USE_GPU:\n",
    "    instance_kind = 'KIND_GPU'\n",
    "else:\n",
    "    instance_kind = 'KIND_CPU'\n",
    "\n",
    "# whether the model is doing classification or regression    \n",
    "if IS_CLASSIFIER:\n",
    "    classifier_string = 'true'\n",
    "else:\n",
    "    classifier_string = 'false'\n",
    "\n",
    "# whether to predict probabilites or not\n",
    "predict_proba = False\n",
    "\n",
    "if predict_proba:\n",
    "    predict_proba_string = 'true'\n",
    "else:\n",
    "    predict_proba_string = 'false'\n",
    "\n",
    "config_text = f\"\"\"backend: \"fil\"\n",
    "max_batch_size: {max_batch_size}\n",
    "input [                                 \n",
    " {{  \n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {NUM_FEATURES} ]                    \n",
    "  }} \n",
    "]\n",
    "output [\n",
    " {{\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "  }}\n",
    "]\n",
    "instance_group [{{ kind: {instance_kind} }}]\n",
    "parameters [\n",
    "  {{\n",
    "    key: \"model_type\"\n",
    "    value: {{ string_value: \"{model_format}\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"predict_proba\"\n",
    "    value: {{ string_value: \"{predict_proba_string}\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"output_class\"\n",
    "    value: {{ string_value: \"{classifier_string}\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"threshold\"\n",
    "    value: {{ string_value: \"0.5\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"storage_type\"\n",
    "    value: {{ string_value: \"AUTO\" }}\n",
    "  }}\n",
    "]\n",
    "\n",
    "dynamic_batching {{}}\"\"\"\n",
    "\n",
    "config_path = os.path.join(FIL_MODEL_DIR, 'config.pbtxt')\n",
    "with open(config_path, 'w') as file_:\n",
    "    file_.write(config_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a09e50a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Conda Env for Preprocessing Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573a801d",
   "metadata": {},
   "source": [
    "Python backend in Triton requires us to use conda environment for any additional dependencies. In this case we are using the Python backend to do preprocessing of the raw data before feeding it into the XGBoost model being run in FIL Backend. Even though we originally used RAPIDS cuDF and cuML to do the data preprocessing here we use Pandas and Scikit-learn as preprocessing dependencies for inference time. We do this for three reasons. \n",
    "* Firstly, to show how to create conda environment for your dependencies and how to package it in [format expected](https://github.com/triton-inference-server/python_backend#2-packaging-the-conda-environment) by Triton's Python backend. \n",
    "* Secondly, by showing preprocessing model running in Python backend on the CPU while the XGBoost runs on the GPU we illustrate how each model in Triton's ensemble pipeline can run on different framework backend, and run on different hardware. \n",
    "* Thirdly, it highlights how the RAPIDS libraries (cuDF, cuML) are compatible with their CPU counterparts (Pandas, Scikit-learn). For e.g. this way we get to show how LabelEncoders created in cuML can be used in Scikit-learn and vice-versa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4a9d8a",
   "metadata": {},
   "source": [
    "We follow the instructions [here](https://github.com/triton-inference-server/python_backend#2-packaging-the-conda-environment) for packaging preprocessing dependencies (here scikit-learn and pandas) to be used in the python backend as conda env file. The [create_prep_env.sh](./create_prep_env.sh) creates the conda environment and then we move it into the python model folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e2a603c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.4\n",
      "  latest version: 4.13.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/preprocessing_env\n",
      "\n",
      "  added / updated specs:\n",
      "    - python=3.8\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _openmp_mutex-4.5          |            2_gnu          23 KB  conda-forge\n",
      "    ca-certificates-2022.6.15  |       ha878542_0         149 KB  conda-forge\n",
      "    libgcc-ng-12.1.0           |      h8d9b700_16         940 KB  conda-forge\n",
      "    libgomp-12.1.0             |      h8d9b700_16         459 KB  conda-forge\n",
      "    libzlib-1.2.12             |       h166bdaf_1          63 KB  conda-forge\n",
      "    openssl-3.0.5              |       h166bdaf_0         2.9 MB  conda-forge\n",
      "    pip-22.1.2                 |     pyhd8ed1ab_0         1.5 MB  conda-forge\n",
      "    python-3.8.13              |ha86cf86_0_cpython        25.2 MB  conda-forge\n",
      "    python_abi-3.8             |           2_cp38           4 KB  conda-forge\n",
      "    readline-8.1.2             |       h0f457ee_0         291 KB  conda-forge\n",
      "    setuptools-63.1.0          |   py38h578d9bd_0         1.3 MB  conda-forge\n",
      "    sqlite-3.39.0              |       h4ff8645_0         1.5 MB  conda-forge\n",
      "    zlib-1.2.12                |       h166bdaf_1          91 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        34.4 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge\n",
      "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu\n",
      "  bzip2              conda-forge/linux-64::bzip2-1.0.8-h7f98852_4\n",
      "  ca-certificates    conda-forge/linux-64::ca-certificates-2022.6.15-ha878542_0\n",
      "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.36.1-hea4e1c9_2\n",
      "  libffi             conda-forge/linux-64::libffi-3.4.2-h7f98852_5\n",
      "  libgcc-ng          conda-forge/linux-64::libgcc-ng-12.1.0-h8d9b700_16\n",
      "  libgomp            conda-forge/linux-64::libgomp-12.1.0-h8d9b700_16\n",
      "  libnsl             conda-forge/linux-64::libnsl-2.0.0-h7f98852_0\n",
      "  libuuid            conda-forge/linux-64::libuuid-2.32.1-h7f98852_1000\n",
      "  libzlib            conda-forge/linux-64::libzlib-1.2.12-h166bdaf_1\n",
      "  ncurses            conda-forge/linux-64::ncurses-6.3-h27087fc_1\n",
      "  openssl            conda-forge/linux-64::openssl-3.0.5-h166bdaf_0\n",
      "  pip                conda-forge/noarch::pip-22.1.2-pyhd8ed1ab_0\n",
      "  python             conda-forge/linux-64::python-3.8.13-ha86cf86_0_cpython\n",
      "  python_abi         conda-forge/linux-64::python_abi-3.8-2_cp38\n",
      "  readline           conda-forge/linux-64::readline-8.1.2-h0f457ee_0\n",
      "  setuptools         conda-forge/linux-64::setuptools-63.1.0-py38h578d9bd_0\n",
      "  sqlite             conda-forge/linux-64::sqlite-3.39.0-h4ff8645_0\n",
      "  tk                 conda-forge/linux-64::tk-8.6.12-h27826a3_0\n",
      "  wheel              conda-forge/noarch::wheel-0.37.1-pyhd8ed1ab_0\n",
      "  xz                 conda-forge/linux-64::xz-5.2.5-h516909a_1\n",
      "  zlib               conda-forge/linux-64::zlib-1.2.12-h166bdaf_1\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "openssl-3.0.5        | 2.9 MB    | ##################################### | 100% \n",
      "sqlite-3.39.0        | 1.5 MB    | ##################################### | 100% \n",
      "pip-22.1.2           | 1.5 MB    | ##################################### | 100% \n",
      "_openmp_mutex-4.5    | 23 KB     | ##################################### | 100% \n",
      "zlib-1.2.12          | 91 KB     | ##################################### | 100% \n",
      "ca-certificates-2022 | 149 KB    | ##################################### | 100% \n",
      "setuptools-63.1.0    | 1.3 MB    | ##################################### | 100% \n",
      "python_abi-3.8       | 4 KB      | ##################################### | 100% \n",
      "libzlib-1.2.12       | 63 KB     | ##################################### | 100% \n",
      "libgcc-ng-12.1.0     | 940 KB    | ##################################### | 100% \n",
      "readline-8.1.2       | 291 KB    | ##################################### | 100% \n",
      "libgomp-12.1.0       | 459 KB    | ##################################### | 100% \n",
      "python-3.8.13        | 25.2 MB   | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate preprocessing_env\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.4\n",
      "  latest version: 4.13.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/preprocessing_env\n",
      "\n",
      "  added / updated specs:\n",
      "    - pandas\n",
      "    - scikit-learn\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    libblas-3.9.0              |15_linux64_openblas          12 KB  conda-forge\n",
      "    libcblas-3.9.0             |15_linux64_openblas          12 KB  conda-forge\n",
      "    libgfortran-ng-12.1.0      |      h69a702a_16          23 KB  conda-forge\n",
      "    libgfortran5-12.1.0        |      hdcd56e2_16         1.8 MB  conda-forge\n",
      "    liblapack-3.9.0            |15_linux64_openblas          12 KB  conda-forge\n",
      "    libstdcxx-ng-12.1.0        |      ha89aaad_16         4.3 MB  conda-forge\n",
      "    numpy-1.23.1               |   py38h3a7f9d9_0         7.1 MB  conda-forge\n",
      "    pandas-1.4.3               |   py38h47df419_0        12.7 MB  conda-forge\n",
      "    scikit-learn-1.1.1         |   py38hf80bbf7_0         8.3 MB  conda-forge\n",
      "    scipy-1.8.1                |   py38h1ee437e_0        25.0 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        59.2 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  joblib             conda-forge/noarch::joblib-1.1.0-pyhd8ed1ab_0\n",
      "  libblas            conda-forge/linux-64::libblas-3.9.0-15_linux64_openblas\n",
      "  libcblas           conda-forge/linux-64::libcblas-3.9.0-15_linux64_openblas\n",
      "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-12.1.0-h69a702a_16\n",
      "  libgfortran5       conda-forge/linux-64::libgfortran5-12.1.0-hdcd56e2_16\n",
      "  liblapack          conda-forge/linux-64::liblapack-3.9.0-15_linux64_openblas\n",
      "  libopenblas        conda-forge/linux-64::libopenblas-0.3.20-pthreads_h78a6416_0\n",
      "  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-12.1.0-ha89aaad_16\n",
      "  numpy              conda-forge/linux-64::numpy-1.23.1-py38h3a7f9d9_0\n",
      "  pandas             conda-forge/linux-64::pandas-1.4.3-py38h47df419_0\n",
      "  python-dateutil    conda-forge/noarch::python-dateutil-2.8.2-pyhd8ed1ab_0\n",
      "  pytz               conda-forge/noarch::pytz-2022.1-pyhd8ed1ab_0\n",
      "  scikit-learn       conda-forge/linux-64::scikit-learn-1.1.1-py38hf80bbf7_0\n",
      "  scipy              conda-forge/linux-64::scipy-1.8.1-py38h1ee437e_0\n",
      "  six                conda-forge/noarch::six-1.16.0-pyh6c4a22f_0\n",
      "  threadpoolctl      conda-forge/noarch::threadpoolctl-3.1.0-pyh8a188c0_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "numpy-1.23.1         | 7.1 MB    | ##################################### | 100% \n",
      "scipy-1.8.1          | 25.0 MB   | ##################################### | 100% \n",
      "libblas-3.9.0        | 12 KB     | ##################################### | 100% \n",
      "liblapack-3.9.0      | 12 KB     | ##################################### | 100% \n",
      "libgfortran-ng-12.1. | 23 KB     | ##################################### | 100% \n",
      "pandas-1.4.3         | 12.7 MB   | ##################################### | 100% \n",
      "scikit-learn-1.1.1   | 8.3 MB    | ##################################### | 100% \n",
      "libcblas-3.9.0       | 12 KB     | ##################################### | 100% \n",
      "libstdcxx-ng-12.1.0  | 4.3 MB    | ##################################### | 100% \n",
      "libgfortran5-12.1.0  | 1.8 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com, https://pypi.ngc.nvidia.com\n",
      "Collecting conda-pack\n",
      "  Downloading conda-pack-0.6.0.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/preprocessing_env/lib/python3.8/site-packages (from conda-pack) (63.1.0)\n",
      "Building wheels for collected packages: conda-pack\n",
      "  Building wheel for conda-pack (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for conda-pack: filename=conda_pack-0.6.0-py2.py3-none-any.whl size=30883 sha256=05cfc9e58e238937af19bbb6e3aced847e2e5fbedd93e9e8efd31c1e717bdfc5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-k9b0cwd_/wheels/56/1b/9e/0da27a4c18349d8f048a8fe87d763d75d3098384e9fa285e45\n",
      "Successfully built conda-pack\n",
      "Installing collected packages: conda-pack\n",
      "Successfully installed conda-pack-0.6.0\n",
      "Collecting packages...\n",
      "Packing environment at '/home/ec2-user/anaconda3/envs/preprocessing_env' to 'preprocessing_env.tar.gz'\n",
      "[########################################] | 100% Completed | 26.5s\n"
     ]
    }
   ],
   "source": [
    "!bash create_prep_env.sh\n",
    "!cp preprocessing_env.tar.gz model_repository/preprocessing/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba2e32b",
   "metadata": {},
   "source": [
    "### Set up Label Encoders and XGBoost model in Model Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddb85641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move label encoders into python preprocessing directory\n",
    "!cp label_encoders.pkl model_repository/preprocessing/1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff257a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move trained xgboost model into fil model directory\n",
    "!mkdir -p model_repository/fil/1\n",
    "!cp xgboost.json model_repository/fil/1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75a7fde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model version directory for ensemble model\n",
    "!mkdir -p model_repository/ensemble/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7651ccca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./fil/\n",
      "./fil/config.pbtxt\n",
      "./fil/1/\n",
      "./fil/1/xgboost.json\n",
      "./preprocessing/\n",
      "./preprocessing/preprocessing_env.tar.gz\n",
      "./preprocessing/config.pbtxt\n",
      "./preprocessing/1/\n",
      "./preprocessing/1/model.py\n",
      "./preprocessing/1/label_encoders.pkl\n",
      "./ensemble/\n",
      "./ensemble/config.pbtxt\n",
      "./ensemble/1/\n"
     ]
    }
   ],
   "source": [
    "!tar --exclude='.ipynb_checkpoints' -czvf model.tar.gz -C model_repository ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc4622b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = sagemaker_session.upload_data(path=\"model.tar.gz\", key_prefix=\"triton-fil-ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38633ff",
   "metadata": {},
   "source": [
    "## Create SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22551549",
   "metadata": {},
   "source": [
    "We start off by creating a sagemaker model from the model files we uploaded to s3 in the previous step.\n",
    "\n",
    "In this step we also provide an additional Environment Variable i.e. `SAGEMAKER_TRITON_DEFAULT_MODEL_NAME` which specifies the name of the model to be loaded by Triton. **The value of this key should match the folder name in the model package uploaded to s3.** This variable is optional in case of a single model. In case of ensemble models, this **key has to be specified** for Triton to startup in SageMaker.\n",
    "\n",
    "Additionally, customers can set `SAGEMAKER_TRITON_BUFFER_MANAGER_THREAD_COUNT` and `SAGEMAKER_TRITON_THREAD_COUNT` for optimizing the thread counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95649816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Arn: arn:aws:sagemaker:us-west-2:354625738399:model/triton-fil-ensemble-2022-07-13-01-48-17\n"
     ]
    }
   ],
   "source": [
    "sm_model_name = \"triton-fil-ensemble-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "container = {\n",
    "    \"Image\": triton_image_uri,\n",
    "    \"ModelDataUrl\": model_uri,\n",
    "    \"Environment\": {\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": \"ensemble\"},\n",
    "}\n",
    "\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27417836",
   "metadata": {},
   "source": [
    "Using the model above, we create an endpoint configuration where we can specify the type and number of instances we want in the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a69409f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Config Arn: arn:aws:sagemaker:us-west-2:354625738399:endpoint-config/triton-fil-ensemble-2022-07-13-01-48-18\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = \"triton-fil-ensemble-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g4dn.4xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e823f660",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to InService once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01ae5fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-west-2:354625738399:endpoint/triton-fil-ensemble-2022-07-13-01-48-18\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = \"triton-fil-ensemble-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10a2267f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-west-2:354625738399:endpoint/triton-fil-ensemble-2022-07-13-01-48-18\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bea132",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bb3c89",
   "metadata": {},
   "source": [
    "Once we have the endpoint running we can use some sample raw data to do an inference using json as the payload format. For inference request format, Triton uses the KFServing community standard [inference protocols.](https://github.com/triton-inference-server/server/blob/main/docs/protocol/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32d8d3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Card</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Time</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Use Chip</th>\n",
       "      <th>Merchant Name</th>\n",
       "      <th>Merchant City</th>\n",
       "      <th>Merchant State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>MCC</th>\n",
       "      <th>Errors?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1904</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>11:08</td>\n",
       "      <td>$16.27</td>\n",
       "      <td>Chip Transaction</td>\n",
       "      <td>-5162038175624867091</td>\n",
       "      <td>Elk Grove</td>\n",
       "      <td>CA</td>\n",
       "      <td>95624.0</td>\n",
       "      <td>5541</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>15:27</td>\n",
       "      <td>$52.87</td>\n",
       "      <td>Online Transaction</td>\n",
       "      <td>-2088492411650162548</td>\n",
       "      <td>ONLINE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4784</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>572</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>17:08</td>\n",
       "      <td>$3.89</td>\n",
       "      <td>Chip Transaction</td>\n",
       "      <td>-2444278202958188094</td>\n",
       "      <td>Pearland</td>\n",
       "      <td>TX</td>\n",
       "      <td>77584.0</td>\n",
       "      <td>5912</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1325</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>08:15</td>\n",
       "      <td>$73.97</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>-5581123930363301609</td>\n",
       "      <td>Bowling Green</td>\n",
       "      <td>KY</td>\n",
       "      <td>42101.0</td>\n",
       "      <td>5311</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1946</td>\n",
       "      <td>3</td>\n",
       "      <td>2005</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>21:09</td>\n",
       "      <td>$46.95</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>-3213879500583660539</td>\n",
       "      <td>Fresno</td>\n",
       "      <td>CA</td>\n",
       "      <td>93725.0</td>\n",
       "      <td>7995</td>\n",
       "      <td>Insufficient Balance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User  Card  Year  Month  Day   Time  Amount            Use Chip  \\\n",
       "0  1904     3  2020      2    8  11:08  $16.27    Chip Transaction   \n",
       "1  1896     1  2008     11    7  15:27  $52.87  Online Transaction   \n",
       "2   572     0  2016     10    8  17:08   $3.89    Chip Transaction   \n",
       "3  1325     0  2014      6    7  08:15  $73.97   Swipe Transaction   \n",
       "4  1946     3  2005      5    8  21:09  $46.95   Swipe Transaction   \n",
       "\n",
       "         Merchant Name  Merchant City Merchant State      Zip   MCC  \\\n",
       "0 -5162038175624867091      Elk Grove             CA  95624.0  5541   \n",
       "1 -2088492411650162548         ONLINE            NaN      NaN  4784   \n",
       "2 -2444278202958188094       Pearland             TX  77584.0  5912   \n",
       "3 -5581123930363301609  Bowling Green             KY  42101.0  5311   \n",
       "4 -3213879500583660539         Fresno             CA  93725.0  7995   \n",
       "\n",
       "                Errors?  \n",
       "0                   NaN  \n",
       "1                   NaN  \n",
       "2                   NaN  \n",
       "3                   NaN  \n",
       "4  Insufficient Balance  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_infer = pd.read_csv(\"data_infer.csv\")\n",
    "data_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1adb7e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "STR_COLUMNS = ['Time',\n",
    " 'Amount',\n",
    " 'Zip',\n",
    " 'MCC',\n",
    " 'Merchant Name',\n",
    " 'Use Chip',\n",
    " 'Merchant City',\n",
    " 'Merchant State',\n",
    " 'Errors?']\n",
    "\n",
    "batch_size = len(data_infer)\n",
    "\n",
    "payload = {}\n",
    "payload[\"inputs\"] = []\n",
    "data_dict = {}\n",
    "for col in data_infer.columns:\n",
    "    data_dict[col] = {}\n",
    "    data_dict[col]['name'] = col\n",
    "    if col in STR_COLUMNS:\n",
    "        data_dict[col]['data'] = data_infer[col].astype(str).tolist()\n",
    "        data_dict[col]['datatype'] = 'BYTES'\n",
    "    else:\n",
    "        data_dict[col]['data'] = data_infer[col].astype('float32').tolist()\n",
    "        data_dict[col]['datatype'] = 'FP32'\n",
    "    data_dict[col]['shape'] = [batch_size, 1]\n",
    "    payload[\"inputs\"].append(data_dict[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eeab7472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': [{'name': 'User',\n",
       "   'data': [1904.0, 1896.0, 572.0, 1325.0, 1946.0],\n",
       "   'datatype': 'FP32',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Card',\n",
       "   'data': [3.0, 1.0, 0.0, 0.0, 3.0],\n",
       "   'datatype': 'FP32',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Year',\n",
       "   'data': [2020.0, 2008.0, 2016.0, 2014.0, 2005.0],\n",
       "   'datatype': 'FP32',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Month',\n",
       "   'data': [2.0, 11.0, 10.0, 6.0, 5.0],\n",
       "   'datatype': 'FP32',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Day',\n",
       "   'data': [8.0, 7.0, 8.0, 7.0, 8.0],\n",
       "   'datatype': 'FP32',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Time',\n",
       "   'data': ['11:08', '15:27', '17:08', '08:15', '21:09'],\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Amount',\n",
       "   'data': ['$16.27', '$52.87', '$3.89', '$73.97', '$46.95'],\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Use Chip',\n",
       "   'data': ['Chip Transaction',\n",
       "    'Online Transaction',\n",
       "    'Chip Transaction',\n",
       "    'Swipe Transaction',\n",
       "    'Swipe Transaction'],\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Merchant Name',\n",
       "   'data': ['-5162038175624867091',\n",
       "    '-2088492411650162548',\n",
       "    '-2444278202958188094',\n",
       "    '-5581123930363301609',\n",
       "    '-3213879500583660539'],\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Merchant City',\n",
       "   'data': ['Elk Grove', 'ONLINE', 'Pearland', 'Bowling Green', 'Fresno'],\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Merchant State',\n",
       "   'data': ['CA', 'nan', 'TX', 'KY', 'CA'],\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Zip',\n",
       "   'data': ['95624.0', 'nan', '77584.0', '42101.0', '93725.0'],\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'MCC',\n",
       "   'data': ['5541', '4784', '5912', '5311', '7995'],\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Errors?',\n",
       "   'data': ['nan', 'nan', 'nan', 'nan', 'Insufficient Balance'],\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [5, 1]}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebe886d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/octet-stream\", Body=json.dumps(payload)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f1c443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_body = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "predictions = response_body['outputs'][0]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eccd2739",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_LABELS = ['NOT FRAUD', 'FRAUD']\n",
    "predictions = [CLASS_LABELS[int(idx)] for idx in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5080852e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOT FRAUD', 'NOT FRAUD', 'NOT FRAUD', 'NOT FRAUD', 'NOT FRAUD']\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c707566",
   "metadata": {},
   "source": [
    "## Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d4fa690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'caed8352-bfaf-40d6-a3f2-4dc7fc63babd',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'caed8352-bfaf-40d6-a3f2-4dc7fc63babd',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Wed, 13 Jul 2022 01:54:19 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_model(ModelName=sm_model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
