{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b15ac3a8",
   "metadata": {},
   "source": [
    "# Pre-processing + XGBoost model inference pipeline with NVIDIA Triton Inference Server on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553e6cc5-2a63-45f9-80fb-2fcc65e38e86",
   "metadata": {},
   "source": [
    "With 22.05 release of [NVIDIA Triton](https://github.com/triton-inference-server/server/) container image on SageMaker you can now use Triton's [Forest Inference Library (FIL) backend](https://github.com/triton-inference-server/fil_backend) to easily serve tree based ML models like XGBoost for high-performance CPU and GPU inference in SageMaker. Using Triton's FIL backend allows you to benefit from the performance optimizations like dynamic batching, concurrent execution which help maximize the utilization of GPU and CPU, further lowering the cost of inference. And the multi-framework support provided by NVIDIA Triton allows you seamlessly deploy tree based ML models alongside deep learning models for fast, unified inference pipelines.\n",
    "\n",
    "Machine Learning applications are complex and can often require data pre-processing. So in this notebook, we will not only deep dive into how to deploy a tree-based ML model like XGBoost using the FIL Backend in Triton on SageMaker endpoint but we will also cover how to implement python-based data pre-processing inference pipeline for your model using the ensemble feature in Triton. This will allow us to send in the raw data from client side and have both data pre-processing and model inference happen in Triton SageMaker endpoint for the optimal inference performance.\n",
    "\n",
    "**Note:** This notebook was tested with the `conda_python3` kernel on an Amazon SageMaker notebook instance of type `g4dn`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8117224d-8b7c-446f-90b3-669def64cf71",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f2d18d-93b6-4b34-9776-689f0bf5eddb",
   "metadata": {},
   "source": [
    "## Forest Inference Library (FIL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb668d-5b88-427e-844d-6499f5cc98af",
   "metadata": {},
   "source": [
    "RAPIDS Forest Inference Library (FIL) is a library to provide high-performance inference for tree-based models. Here are some important FIL features:\n",
    "\n",
    "* Supports XGBoost, LightGBM, cuML RandomForest, and Scikit Learn Random Forest\n",
    "* No conversion needed for XGBoost and LightGBM. SKLearn or cuML pickle models need to be converted to Treelite's binary checkpoint format \n",
    "* SKLearn Random Forest is supported for single-output regression and multi-class classification\n",
    "* Both CPU and GPU are supported\n",
    "\n",
    "Below we show benchmark highlighting FIL's throughput performance against CPU XGBoost.\n",
    "\n",
    "<img src=\"./images/fil_benchmark.png\" alt=\"fil-benchmark\" width=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971c535f-2f2c-4c35-b4ad-8238503262ca",
   "metadata": {},
   "source": [
    "## Triton FIL Backend\n",
    "FIL is available as a backend in Triton with all of its features to allow for serving XGBoost, LightGBM and RandomForest models both on CPU and GPU with high performance. Here are some important features of the FIL Backend:\n",
    "\n",
    "* **Shapley Value Support (GPU)**: GPU Shapley Values are supported for Model Explainability\n",
    "* **Categorical Feature Support**: Models trained on categorical features fully supported.\n",
    "* **CPU Optimizations**: Optimized CPU mode offers faster execution than native XGBoost.\n",
    "\n",
    "To learn more about FIL Backend's features please see the [FAQ Notebook](https://github.com/triton-inference-server/fil_backend/blob/fea-faq_nb/notebooks/faq/FAQs.ipynb) and [Triton FIL Backend GitHub](https://github.com/triton-inference-server/fil_backend/tree/main)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fbd827-edf2-46af-8412-0be9d612f378",
   "metadata": {},
   "source": [
    "## Triton Model Ensemble Feature\n",
    "Triton Inference Server greatly simplifies the deployment of AI models at scale in production. Triton Server comes with a convenient solution that simplifies building pre-processing and post-processing pipelines. Triton Server platform provides the ensemble scheduler, which is responsible for pipelining models participating in the inference process while ensuring efficiency and optimizing throughput. Using ensemble models can avoid the overhead of transferring intermediate tensors and minimize the number of requests that must be sent to Triton.\n",
    "\n",
    "<img src=\"./images/triton-ensemble.png\" alt=\"triton-ensemble\" width=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3353763-b166-403e-86f8-2d2f2e2f6d77",
   "metadata": {},
   "source": [
    "In this notebook we will be showing how to use ensemble feature for building a pipeline of data preprocessing with XGBoost model inference and you can extrapolate from it to add custom postprocessing to the pipeline if you need to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd4fa0e",
   "metadata": {},
   "source": [
    "## Set up Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6de40f",
   "metadata": {},
   "source": [
    "We begin by setting up the required environment. We will install the dependencies required to package our model pipeline and run inferences using Triton server.\n",
    "\n",
    "Also define the IAM role that will give SageMaker access to the model artifacts and the NVIDIA Triton ECR image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57035a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU pip awscli boto3 sagemaker --quiet\n",
    "!pip install nvidia-pyindex --quiet\n",
    "!pip install tritonclient[http] --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2017a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "import time\n",
    "import os\n",
    "from sagemaker import get_execution_role\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796628a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd33ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise(\"UNSUPPORTED REGION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962f0513",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "triton_image_uri = \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.05-py3\".format(\n",
    "    account_id=account_id_map[region], region=region, base=base\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4d2c74-de16-4d0d-afe7-370aff0c6d78",
   "metadata": {},
   "source": [
    "## Set up pre-processing with Triton Python Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8322472-dbfd-420c-8472-566265bc6cad",
   "metadata": {},
   "source": [
    "We will be using Triton's [Python Backend](https://github.com/triton-inference-server/python_backend) to perform the same tabular data preprocessing that we did in [first notebook](1_prep_rapids_train_xgb.ipynb) but now during inference time for raw data requests coming into the server. The Python backend enables pre-process, post-processing and any other custom logic to be implemented in Python and served with Triton."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d439b-e383-4870-b205-38ee483329c5",
   "metadata": {},
   "source": [
    "Using Triton on SageMaker requires us to first set up a model repository folder containing the models we want to serve. We have already set up model for python data preprocessing called `preprocessing` in the `model_repository`.\n",
    "\n",
    "<img src=\"./images/preprocessing_model.png\" alt=\"preprocessing-model\" width=\"200\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df10122f-b7b9-49bc-958a-8c6c907151c1",
   "metadata": {},
   "source": [
    "Now Triton has specific requirements for model repository layout. Each model in Triton must have at least one numeric sub-directory representing a version of the model. Here that is `1` representing version 1 of our python preprocessing model. Each model is executed by a specific backend so within each version sub-directory there must be the model artifact required by that backend. For example, for PyTorch backend a `model.pt` file is required. For more details please see [model configuration doc](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md). Here, we are using the Python backend and it requires the python file you are serving to be called `model.py` and the file needs to implement [certain functions](https://github.com/triton-inference-server/python_backend#usage).\n",
    "\n",
    "[Our model.py](model_repository/preprocessing/1/model.py) python file we are using here implements all the tabular data preprocessing logic to convert raw data into features that can be fed into our XGBoost model.\n",
    "\n",
    "Every Triton model must also provide a `config.pbtxt` file describing the model configuration To learn more about the config settings please see [this](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md) page. [Our config.pbtxt](model_repository/preprocessing/config.pbtxt) specifies the backend as `python` and specifies all the input columns for raw data along with preprocessed output that consists of 15 features. We also specify we want to run this python preprocessing model on the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b52dc-e522-482b-b9b2-15795b665307",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Conda Env for Preprocessing Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8c303c-e2c2-4dbf-9fbf-e7808e115c64",
   "metadata": {},
   "source": [
    "Python backend in Triton requires us to use conda environment for any additional dependencies. In this case we are using the Python backend to do preprocessing of the raw data before feeding it into the XGBoost model being run in FIL Backend. Even though we originally used RAPIDS cuDF and cuML to do the data preprocessing here we use Pandas and Scikit-learn as preprocessing dependencies for inference time. We do this for three reasons. \n",
    "* Firstly, to show how to create conda environment for your dependencies and how to package it in [format expected](https://github.com/triton-inference-server/python_backend#2-packaging-the-conda-environment) by Triton's Python backend. \n",
    "* Secondly, by showing preprocessing model running in Python backend on the CPU while the XGBoost runs on the GPU in FIL Backend we illustrate how each model in Triton's ensemble pipeline can run on different framework backend, and run on different hardware with different configurations\n",
    "* Thirdly, it highlights how the RAPIDS libraries (cuDF, cuML) are compatible with their CPU counterparts (Pandas, Scikit-learn). For e.g. this way we get to show how LabelEncoders created in cuML can be used in Scikit-learn and vice-versa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a065b52-b623-48a0-b16e-c2d92a68a0c4",
   "metadata": {},
   "source": [
    "We follow the instructions [here](https://github.com/triton-inference-server/python_backend#2-packaging-the-conda-environment) for packaging preprocessing dependencies (here scikit-learn and pandas) to be used in the python backend as conda env tar file. The bash script [create_prep_env.sh](./create_prep_env.sh) creates the conda environment tar file and then we move it into the preprocessing model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcbe1773-00fe-447c-bf36-b445395356ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.10.3\n",
      "  latest version: 4.13.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda/envs/preprocessing_env\n",
      "\n",
      "  added / updated specs:\n",
      "    - python=3.8\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _libgcc_mutex-0.1          |      conda_forge           3 KB  conda-forge\n",
      "    _openmp_mutex-4.5          |            2_gnu          23 KB  conda-forge\n",
      "    bzip2-1.0.8                |       h7f98852_4         484 KB  conda-forge\n",
      "    ca-certificates-2022.6.15  |       ha878542_0         149 KB  conda-forge\n",
      "    ld_impl_linux-64-2.36.1    |       hea4e1c9_2         667 KB  conda-forge\n",
      "    libffi-3.4.2               |       h7f98852_5          57 KB  conda-forge\n",
      "    libgcc-ng-12.1.0           |      h8d9b700_16         940 KB  conda-forge\n",
      "    libgomp-12.1.0             |      h8d9b700_16         459 KB  conda-forge\n",
      "    libnsl-2.0.0               |       h7f98852_0          31 KB  conda-forge\n",
      "    libuuid-2.32.1             |    h7f98852_1000          28 KB  conda-forge\n",
      "    libzlib-1.2.12             |       h166bdaf_2          63 KB  conda-forge\n",
      "    ncurses-6.3                |       h27087fc_1        1002 KB  conda-forge\n",
      "    openssl-3.0.5              |       h166bdaf_0         2.9 MB  conda-forge\n",
      "    pip-22.1.2                 |     pyhd8ed1ab_0         1.5 MB  conda-forge\n",
      "    python-3.8.13              |ha86cf86_0_cpython        25.2 MB  conda-forge\n",
      "    python_abi-3.8             |           2_cp38           4 KB  conda-forge\n",
      "    readline-8.1.2             |       h0f457ee_0         291 KB  conda-forge\n",
      "    setuptools-63.2.0          |   py38h578d9bd_0         1.3 MB  conda-forge\n",
      "    sqlite-3.39.1              |       h4ff8645_0         1.5 MB  conda-forge\n",
      "    tk-8.6.12                  |       h27826a3_0         3.3 MB  conda-forge\n",
      "    wheel-0.37.1               |     pyhd8ed1ab_0          31 KB  conda-forge\n",
      "    xz-5.2.5                   |       h516909a_1         343 KB  conda-forge\n",
      "    zlib-1.2.12                |       h166bdaf_2          91 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        40.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge\n",
      "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu\n",
      "  bzip2              conda-forge/linux-64::bzip2-1.0.8-h7f98852_4\n",
      "  ca-certificates    conda-forge/linux-64::ca-certificates-2022.6.15-ha878542_0\n",
      "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.36.1-hea4e1c9_2\n",
      "  libffi             conda-forge/linux-64::libffi-3.4.2-h7f98852_5\n",
      "  libgcc-ng          conda-forge/linux-64::libgcc-ng-12.1.0-h8d9b700_16\n",
      "  libgomp            conda-forge/linux-64::libgomp-12.1.0-h8d9b700_16\n",
      "  libnsl             conda-forge/linux-64::libnsl-2.0.0-h7f98852_0\n",
      "  libuuid            conda-forge/linux-64::libuuid-2.32.1-h7f98852_1000\n",
      "  libzlib            conda-forge/linux-64::libzlib-1.2.12-h166bdaf_2\n",
      "  ncurses            conda-forge/linux-64::ncurses-6.3-h27087fc_1\n",
      "  openssl            conda-forge/linux-64::openssl-3.0.5-h166bdaf_0\n",
      "  pip                conda-forge/noarch::pip-22.1.2-pyhd8ed1ab_0\n",
      "  python             conda-forge/linux-64::python-3.8.13-ha86cf86_0_cpython\n",
      "  python_abi         conda-forge/linux-64::python_abi-3.8-2_cp38\n",
      "  readline           conda-forge/linux-64::readline-8.1.2-h0f457ee_0\n",
      "  setuptools         conda-forge/linux-64::setuptools-63.2.0-py38h578d9bd_0\n",
      "  sqlite             conda-forge/linux-64::sqlite-3.39.1-h4ff8645_0\n",
      "  tk                 conda-forge/linux-64::tk-8.6.12-h27826a3_0\n",
      "  wheel              conda-forge/noarch::wheel-0.37.1-pyhd8ed1ab_0\n",
      "  xz                 conda-forge/linux-64::xz-5.2.5-h516909a_1\n",
      "  zlib               conda-forge/linux-64::zlib-1.2.12-h166bdaf_2\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "setuptools-63.2.0    | 1.3 MB    | ##################################### | 100% \n",
      "_openmp_mutex-4.5    | 23 KB     | ##################################### | 100% \n",
      "ncurses-6.3          | 1002 KB   | ##################################### | 100% \n",
      "wheel-0.37.1         | 31 KB     | ##################################### | 100% \n",
      "sqlite-3.39.1        | 1.5 MB    | ##################################### | 100% \n",
      "_libgcc_mutex-0.1    | 3 KB      | ##################################### | 100% \n",
      "libuuid-2.32.1       | 28 KB     | ##################################### | 100% \n",
      "libffi-3.4.2         | 57 KB     | ##################################### | 100% \n",
      "openssl-3.0.5        | 2.9 MB    | ##################################### | 100% \n",
      "xz-5.2.5             | 343 KB    | ##################################### | 100% \n",
      "pip-22.1.2           | 1.5 MB    | ##################################### | 100% \n",
      "python-3.8.13        | 25.2 MB   | ##################################### | 100% \n",
      "libnsl-2.0.0         | 31 KB     | ##################################### | 100% \n",
      "bzip2-1.0.8          | 484 KB    | ##################################### | 100% \n",
      "libgomp-12.1.0       | 459 KB    | ##################################### | 100% \n",
      "libzlib-1.2.12       | 63 KB     | ##################################### | 100% \n",
      "tk-8.6.12            | 3.3 MB    | ##################################### | 100% \n",
      "ld_impl_linux-64-2.3 | 667 KB    | ##################################### | 100% \n",
      "zlib-1.2.12          | 91 KB     | ##################################### | 100% \n",
      "python_abi-3.8       | 4 KB      | ##################################### | 100% \n",
      "libgcc-ng-12.1.0     | 940 KB    | ##################################### | 100% \n",
      "readline-8.1.2       | 291 KB    | ##################################### | 100% \n",
      "ca-certificates-2022 | 149 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate preprocessing_env\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "create_prep_env.sh: line 4: /root/anaconda3/etc/profile.d/conda.sh: No such file or directory\n",
      "\n",
      "CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\n",
      "To initialize your shell, run\n",
      "\n",
      "    $ conda init <SHELL_NAME>\n",
      "\n",
      "Currently supported shells are:\n",
      "  - bash\n",
      "  - fish\n",
      "  - tcsh\n",
      "  - xonsh\n",
      "  - zsh\n",
      "  - powershell\n",
      "\n",
      "See 'conda init --help' for more information and options.\n",
      "\n",
      "IMPORTANT: You may need to close and restart your shell after running 'conda init'.\n",
      "\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.10.3\n",
      "  latest version: 4.13.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda/envs/rapids\n",
      "\n",
      "  added / updated specs:\n",
      "    - pandas\n",
      "    - scikit-learn\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2022.6.15          |   py38h578d9bd_0         155 KB  conda-forge\n",
      "    scikit-learn-1.1.1         |   py38hf80bbf7_0         8.3 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         8.5 MB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                      2021.10.8-ha878542_0 --> 2022.6.15-ha878542_0\n",
      "  certifi                          2021.10.8-py38h578d9bd_2 --> 2022.6.15-py38h578d9bd_0\n",
      "  scikit-learn                         1.0.2-py38h1561384_0 --> 1.1.1-py38hf80bbf7_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "certifi-2022.6.15    | 155 KB    | ##################################### | 100% \n",
      "scikit-learn-1.1.1   | 8.3 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting conda-pack\n",
      "  Downloading conda-pack-0.6.0.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/envs/rapids/lib/python3.8/site-packages (from conda-pack) (59.8.0)\n",
      "Building wheels for collected packages: conda-pack\n",
      "  Building wheel for conda-pack (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for conda-pack: filename=conda_pack-0.6.0-py2.py3-none-any.whl size=30905 sha256=9969cfe43d9c4e6b258012833829f49d42a6c3ed2c553e97f583fecf15066337\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-xm388riw/wheels/56/1b/9e/0da27a4c18349d8f048a8fe87d763d75d3098384e9fa285e45\n",
      "Successfully built conda-pack\n",
      "Installing collected packages: conda-pack\n",
      "Successfully installed conda-pack-0.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting packages...\n",
      "CondaPackError: \n",
      "Files managed by conda were found to have been deleted/overwritten in the\n",
      "following packages:\n",
      "\n",
      "- pip 22.0.4:\n",
      "    lib/python3.8/site-packages/pip-22.0.4-py3.10.egg-info/PKG-INFO\n",
      "    lib/python3.8/site-packages/pip-22.0.4-py3.10.egg-info/SOURCES.txt\n",
      "    lib/python3.8/site-packages/pip-22.0.4-py3.10.egg-info/dependency_links.txt\n",
      "    + 19 others\n",
      "- nodejs 14.18.3:\n",
      "    lib/node_modules/npm/.licensee.json\n",
      "    lib/node_modules/npm/.mailmap\n",
      "    lib/node_modules/npm/.npmignore\n",
      "    + 3081 others\n",
      "- pyyaml 6.0:\n",
      "    lib/python3.8/site-packages/PyYAML-6.0.dist-info/INSTALLER\n",
      "    lib/python3.8/site-packages/PyYAML-6.0.dist-info/LICENSE\n",
      "    lib/python3.8/site-packages/PyYAML-6.0.dist-info/METADATA\n",
      "    + 5 others\n",
      "\n",
      "This is usually due to `pip` uninstalling or clobbering conda managed files,\n",
      "resulting in an inconsistent environment. Please check your environment for\n",
      "conda/pip conflicts using `conda list`, and fix the environment by ensuring\n",
      "only one version of each package is installed (conda preferred).\n",
      "cp: cannot stat 'preprocessing_env.tar.gz': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!bash create_prep_env.sh\n",
    "!cp preprocessing_env.tar.gz model_repository/preprocessing/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3346b806-3e56-4ba3-b65c-0313ceef4472",
   "metadata": {},
   "source": [
    "After creating the tar file from the conda environment and placing it in model folder, you need to tell Python backend to use that environment for your model. We do this by including the lines below in the model `config.pbtxt` file:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14f9fde-c936-45ed-9348-20e5bc5ef375",
   "metadata": {},
   "source": [
    "```\n",
    "parameters: {\n",
    "  key: \"EXECUTION_ENV_PATH\",\n",
    "  value: {string_value: \"$$TRITON_MODEL_DIRECTORY/preprocessing_env.tar.gz\"}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fced27-de71-4ce5-9adf-df7b1f323617",
   "metadata": {},
   "source": [
    "Here, `$$TRITON_MODEL_DIRECTORY` helps provide environment path relative to the model folder in model repository and is resolved to `$pwd/model_repository/preprocessing`. Finally `preprocessing_env.tar.gz` is the name we gave to our conda env file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1746f83-afa6-4ae4-995d-17833aaf12a2",
   "metadata": {},
   "source": [
    "### Set up Label Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a9ad2-db42-4986-a09e-eb40623747b3",
   "metadata": {},
   "source": [
    "We also move the label encoders we had serialized eariler into `preprocessing` model folder so that we can use them to encode raw data categorical features at inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17c23963-d50a-489d-b946-07f762081708",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp label_encoders.pkl model_repository/preprocessing/1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269dc9da-d1af-46f2-a20e-f62915e95427",
   "metadata": {},
   "source": [
    "## Set up Tree-based ML Model for FIL Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d63682-c06e-4b10-b9f2-044f3c0750a3",
   "metadata": {},
   "source": [
    "Next, we set up the model directory for tree-based ML model like XGBoost which will be using FIL Backend.\n",
    "\n",
    "The expected layout for model directory is similar to the one we showed above:\n",
    "\n",
    "<img src=\"./images/fil_model.png\" alt=\"fil-model\" width=\"200\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19e1ba-1d43-4734-988b-1ccb58875230",
   "metadata": {},
   "source": [
    "Here `fil` is the name of the model. We can give it a different name like xgboost if we want to. `1` is the version sub-directory which contains the model artifact, in this case it's the `xgboost.json` model that we saved at the end of [first notebook](1_prep_rapids_train_xgb.ipynb). Let's create this expected layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbecd82-7dc9-4dbc-b0e0-810512f0a595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move saved xgboost model into fil model directory\n",
    "!mkdir -p model_repository/fil/1\n",
    "!cp xgboost.json model_repository/fil/1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78683daa-7bfb-4f2f-ab19-4d2dd731fba8",
   "metadata": {},
   "source": [
    "And then finally we need to have configuration file `config.pbtxt` describing the model configuration for tree-based ML model so that FIL Backend in Triton can understand how to serve it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0bdab0-6c52-4753-bc54-7367142a68f1",
   "metadata": {},
   "source": [
    "### Create Config File for FIL Backend Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501def48-b052-4fa5-8776-afb06818617b",
   "metadata": {},
   "source": [
    "You can read about all generic Triton configuration options [here](https://github.com/triton-inference-server/server/blob/master/docs/model_configuration.md) and about configuration options specific to the FIL backend [here](https://github.com/triton-inference-server/fil_backend#configuration), but we will focus on just a few of the most common and relevant options in this example. Below are general descriptions of these options:\n",
    "\n",
    "* **max_batch_size:** The maximum batch size that can be passed to this model. In general, the only limit on the size of batches passed to a FIL backend is the memory available with which to process them. \n",
    "* **input:** Options in this section tell Triton the number of features to expect for each input sample.\n",
    "* **output:** Options in this section tell Triton how many output values there will be for each sample. If the \"predict_proba\" option (described further on) is set to true, then a probability value will be returned for each class. Otherwise, a single value will be returned indicating the class predicted for the given sample.\n",
    "* **instance_group:** This determines how many instances of this model will be created and whether they will use the GPU or CPU.\n",
    "* **model_type:** A string indicating what format the model is in (\"xgboost_json\" in this example, but \"xgboost\", \"lightgbm\", and \"tl_checkpoint\" are valid formats as well).\n",
    "* **predict_proba:** If set to true, probability values will be returned for each class rather than just a class prediction.\n",
    "* **output_class:** True for classification models, false for regression models.\n",
    "* **threshold:** A score threshold for determining classification. When output_class is set to true, this must be provided, although it will not be used if predict_proba is also set to true.\n",
    "* **storage_type:** In general, using \"AUTO\" for this setting should meet most usecases. If \"AUTO\" storage is selected, FIL will load the model using either a sparse or dense representation based on the approximate size of the model. In some cases, you may want to explicitly set this to \"SPARSE\" in order to reduce the memory footprint of large models.\n",
    "\n",
    "Here we have 15 input features and 2 classes (FRAUD, NOT FRAUD) that we are doing classification for in our XGBoost Model. Based on this information, let's set up FIL Backend configuration file for our tree-based model for serving on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b61b7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "FIL_MODEL_DIR = './model_repository/fil'\n",
    "\n",
    "# Maximum size in bytes for input and output arrays. If you are\n",
    "# using Triton 21.11 or higher, all memory allocations will make\n",
    "# use of Triton's memory pool, which has a default size of\n",
    "# 67_108_864 bytes\n",
    "MAX_MEMORY_BYTES = 60_000_000\n",
    "NUM_FEATURES = 15\n",
    "NUM_CLASSES = 2\n",
    "bytes_per_sample = (NUM_FEATURES + NUM_CLASSES) * 4\n",
    "max_batch_size = MAX_MEMORY_BYTES // bytes_per_sample\n",
    "\n",
    "IS_CLASSIFIER = True\n",
    "model_format = 'xgboost_json'\n",
    "\n",
    "# Select deployment hardware (GPU or CPU)\n",
    "if USE_GPU:\n",
    "    instance_kind = 'KIND_GPU'\n",
    "else:\n",
    "    instance_kind = 'KIND_CPU'\n",
    "\n",
    "# whether the model is doing classification or regression    \n",
    "if IS_CLASSIFIER:\n",
    "    classifier_string = 'true'\n",
    "else:\n",
    "    classifier_string = 'false'\n",
    "\n",
    "# whether to predict probabilites or not\n",
    "predict_proba = False\n",
    "\n",
    "if predict_proba:\n",
    "    predict_proba_string = 'true'\n",
    "else:\n",
    "    predict_proba_string = 'false'\n",
    "\n",
    "config_text = f\"\"\"backend: \"fil\"\n",
    "max_batch_size: {max_batch_size}\n",
    "input [                                 \n",
    " {{  \n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {NUM_FEATURES} ]                    \n",
    "  }} \n",
    "]\n",
    "output [\n",
    " {{\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "  }}\n",
    "]\n",
    "instance_group [{{ kind: {instance_kind} }}]\n",
    "parameters [\n",
    "  {{\n",
    "    key: \"model_type\"\n",
    "    value: {{ string_value: \"{model_format}\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"predict_proba\"\n",
    "    value: {{ string_value: \"{predict_proba_string}\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"output_class\"\n",
    "    value: {{ string_value: \"{classifier_string}\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"threshold\"\n",
    "    value: {{ string_value: \"0.5\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"storage_type\"\n",
    "    value: {{ string_value: \"AUTO\" }}\n",
    "  }}\n",
    "]\n",
    "\n",
    "dynamic_batching {{}}\"\"\"\n",
    "\n",
    "config_path = os.path.join(FIL_MODEL_DIR, 'config.pbtxt')\n",
    "with open(config_path, 'w') as file_:\n",
    "    file_.write(config_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21221d78-717f-4464-9404-61c4862e2620",
   "metadata": {},
   "source": [
    "### Set up Inference Pipeline of Data Preprocessing Python Backend and FIL Backend using Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aea8f6c-da85-4d90-b9b4-bce7766c138a",
   "metadata": {},
   "source": [
    "Now we are ready to set up the inference pipeline for data preprocessing and tree-based model inference using [ensemble model](https://github.com/triton-inference-server/server/blob/main/docs/architecture.md#ensemble-models). An ensemble model represents a pipeline of one or more models and the connection of input and output tensors between those models. Here we use the ensemble model to build a pipeline of Data Preprocessing in Python backend followed by XGBoost in FIL Backend. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ec4b17-3864-4b62-8a4f-1abe0ad4e3e2",
   "metadata": {},
   "source": [
    "The expected layout for `ensemble` model directory is similar to the ones we showed above:\n",
    "\n",
    "<img src=\"./images/ensemble_model.png\" alt=\"ensemble-model\" width=\"200\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f575d1a-8a65-4ae2-8387-6c8fc0cb6202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model version directory for ensemble model\n",
    "!mkdir -p model_repository/ensemble/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80eeda4-1d06-465f-a5d6-9d7551d61a2b",
   "metadata": {},
   "source": [
    "We created the [config.pbtxt](model_repository/ensemble/config.pbtxt) following the guidance on [ensemble doc](https://github.com/triton-inference-server/server/blob/main/docs/architecture.md#ensemble-models). Importantly, we need to set up the ensemble scheduler in [config.pbtxt](model_repository/ensemble/config.pbtxt) which specifies the dataflow between models within the ensemble. The ensemble scheduler collects the output tensors in each step, provides them as input tensors for other steps according to the specification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec15697b",
   "metadata": {},
   "source": [
    "## Package model repository and upload to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d78bf40-415d-4fd8-86ad-bea9c9a10b78",
   "metadata": {},
   "source": [
    "Finally, we end up with the following model repository directory structure, containing a Python preprocessing model and it dependencies along with XGBoost FIL model, and the model ensemble.\n",
    "\n",
    "<img src=\"./images/model_repo.png\" alt=\"model-repo\" width=\"300\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c9532f-60d7-42e7-b231-e626e5417fcd",
   "metadata": {},
   "source": [
    "We will package this up as `model.tar.gz` for uploading it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7651ccca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./fil/\n",
      "./fil/config.pbtxt\n",
      "./fil/1/\n",
      "./fil/1/xgboost.json\n",
      "./preprocessing/\n",
      "./preprocessing/preprocessing_env.tar.gz\n",
      "./preprocessing/config.pbtxt\n",
      "./preprocessing/1/\n",
      "./preprocessing/1/model.py\n",
      "./preprocessing/1/label_encoders.pkl\n",
      "./ensemble/\n",
      "./ensemble/config.pbtxt\n",
      "./ensemble/1/\n"
     ]
    }
   ],
   "source": [
    "!tar --exclude='.ipynb_checkpoints' -czvf model.tar.gz -C model_repository ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc4622b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = sagemaker_session.upload_data(path=\"model.tar.gz\", key_prefix=\"triton-fil-ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38633ff",
   "metadata": {},
   "source": [
    "## Create SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22551549",
   "metadata": {},
   "source": [
    "We start off by creating a sagemaker model from the model repository we uploaded to s3 in the previous step.\n",
    "\n",
    "In this step we also provide an additional Environment Variable i.e. `SAGEMAKER_TRITON_DEFAULT_MODEL_NAME` which specifies the name of the model to be loaded by Triton. **The value of this key should match the folder name in the model package uploaded to s3.** This variable is optional in case of a single model. In case of ensemble models, this **key has to be specified** for Triton to startup in SageMaker.\n",
    "\n",
    "Additionally, customers can set `SAGEMAKER_TRITON_BUFFER_MANAGER_THREAD_COUNT` and `SAGEMAKER_TRITON_THREAD_COUNT` for optimizing the thread counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95649816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Arn: arn:aws:sagemaker:us-west-2:354625738399:model/triton-fil-ensemble-2022-07-13-01-48-17\n"
     ]
    }
   ],
   "source": [
    "sm_model_name = \"triton-fil-ensemble-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "container = {\n",
    "    \"Image\": triton_image_uri,\n",
    "    \"ModelDataUrl\": model_uri,\n",
    "    \"Environment\": {\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": \"ensemble\"},\n",
    "}\n",
    "\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27417836",
   "metadata": {},
   "source": [
    "Using the model above, we create an endpoint configuration where we can specify the type and number of instances we want in the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a69409f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Config Arn: arn:aws:sagemaker:us-west-2:354625738399:endpoint-config/triton-fil-ensemble-2022-07-13-01-48-18\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = \"triton-fil-ensemble-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g4dn.4xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e823f660",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to InService once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01ae5fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-west-2:354625738399:endpoint/triton-fil-ensemble-2022-07-13-01-48-18\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = \"triton-fil-ensemble-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10a2267f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-west-2:354625738399:endpoint/triton-fil-ensemble-2022-07-13-01-48-18\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bea132",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bb3c89",
   "metadata": {},
   "source": [
    "Once we have the endpoint running we can use some sample raw data to do an inference using json as the payload format. For inference request format, Triton uses the KFServing community standard [inference protocols.](https://github.com/triton-inference-server/server/blob/main/docs/protocol/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32d8d3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Card</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Time</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Use Chip</th>\n",
       "      <th>Merchant Name</th>\n",
       "      <th>Merchant City</th>\n",
       "      <th>Merchant State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>MCC</th>\n",
       "      <th>Errors?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1904</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>11:08</td>\n",
       "      <td>$16.27</td>\n",
       "      <td>Chip Transaction</td>\n",
       "      <td>-5162038175624867091</td>\n",
       "      <td>Elk Grove</td>\n",
       "      <td>CA</td>\n",
       "      <td>95624.0</td>\n",
       "      <td>5541</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1896</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>15:27</td>\n",
       "      <td>$52.87</td>\n",
       "      <td>Online Transaction</td>\n",
       "      <td>-2088492411650162548</td>\n",
       "      <td>ONLINE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4784</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>572</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>17:08</td>\n",
       "      <td>$3.89</td>\n",
       "      <td>Chip Transaction</td>\n",
       "      <td>-2444278202958188094</td>\n",
       "      <td>Pearland</td>\n",
       "      <td>TX</td>\n",
       "      <td>77584.0</td>\n",
       "      <td>5912</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1325</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>08:15</td>\n",
       "      <td>$73.97</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>-5581123930363301609</td>\n",
       "      <td>Bowling Green</td>\n",
       "      <td>KY</td>\n",
       "      <td>42101.0</td>\n",
       "      <td>5311</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1946</td>\n",
       "      <td>3</td>\n",
       "      <td>2005</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>21:09</td>\n",
       "      <td>$46.95</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>-3213879500583660539</td>\n",
       "      <td>Fresno</td>\n",
       "      <td>CA</td>\n",
       "      <td>93725.0</td>\n",
       "      <td>7995</td>\n",
       "      <td>Insufficient Balance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User  Card  Year  Month  Day   Time  Amount            Use Chip  \\\n",
       "0  1904     3  2020      2    8  11:08  $16.27    Chip Transaction   \n",
       "1  1896     1  2008     11    7  15:27  $52.87  Online Transaction   \n",
       "2   572     0  2016     10    8  17:08   $3.89    Chip Transaction   \n",
       "3  1325     0  2014      6    7  08:15  $73.97   Swipe Transaction   \n",
       "4  1946     3  2005      5    8  21:09  $46.95   Swipe Transaction   \n",
       "\n",
       "         Merchant Name  Merchant City Merchant State      Zip   MCC  \\\n",
       "0 -5162038175624867091      Elk Grove             CA  95624.0  5541   \n",
       "1 -2088492411650162548         ONLINE            NaN      NaN  4784   \n",
       "2 -2444278202958188094       Pearland             TX  77584.0  5912   \n",
       "3 -5581123930363301609  Bowling Green             KY  42101.0  5311   \n",
       "4 -3213879500583660539         Fresno             CA  93725.0  7995   \n",
       "\n",
       "                Errors?  \n",
       "0                   NaN  \n",
       "1                   NaN  \n",
       "2                   NaN  \n",
       "3                   NaN  \n",
       "4  Insufficient Balance  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_infer = pd.read_csv(\"data_infer.csv\")\n",
    "data_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1adb7e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "STR_COLUMNS = ['Time',\n",
    " 'Amount',\n",
    " 'Zip',\n",
    " 'MCC',\n",
    " 'Merchant Name',\n",
    " 'Use Chip',\n",
    " 'Merchant City',\n",
    " 'Merchant State',\n",
    " 'Errors?']\n",
    "\n",
    "batch_size = len(data_infer)\n",
    "\n",
    "payload = {}\n",
    "payload[\"inputs\"] = []\n",
    "data_dict = {}\n",
    "for col in data_infer.columns:\n",
    "    data_dict[col] = {}\n",
    "    data_dict[col]['name'] = col\n",
    "    if col in STR_COLUMNS:\n",
    "        data_dict[col]['data'] = data_infer[col].astype(str).tolist()\n",
    "        data_dict[col]['datatype'] = 'BYTES'\n",
    "    else:\n",
    "        data_dict[col]['data'] = data_infer[col].astype('float32').tolist()\n",
    "        data_dict[col]['datatype'] = 'FP32'\n",
    "    data_dict[col]['shape'] = [batch_size, 1]\n",
    "    payload[\"inputs\"].append(data_dict[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eeab7472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': [{'name': 'User',\n",
       "   'data': [1904.0, 1896.0, 572.0, 1325.0, 1946.0],\n",
       "   'datatype': 'FP32',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Card',\n",
       "   'data': [3.0, 1.0, 0.0, 0.0, 3.0],\n",
       "   'datatype': 'FP32',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Year',\n",
       "   'data': [2020.0, 2008.0, 2016.0, 2014.0, 2005.0],\n",
       "   'datatype': 'FP32',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Month',\n",
       "   'data': [2.0, 11.0, 10.0, 6.0, 5.0],\n",
       "   'datatype': 'FP32',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Day',\n",
       "   'data': [8.0, 7.0, 8.0, 7.0, 8.0],\n",
       "   'datatype': 'FP32',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Time',\n",
       "   'data': ['11:08', '15:27', '17:08', '08:15', '21:09'],\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Amount',\n",
       "   'data': ['$16.27', '$52.87', '$3.89', '$73.97', '$46.95'],\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Use Chip',\n",
       "   'data': ['Chip Transaction',\n",
       "    'Online Transaction',\n",
       "    'Chip Transaction',\n",
       "    'Swipe Transaction',\n",
       "    'Swipe Transaction'],\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Merchant Name',\n",
       "   'data': ['-5162038175624867091',\n",
       "    '-2088492411650162548',\n",
       "    '-2444278202958188094',\n",
       "    '-5581123930363301609',\n",
       "    '-3213879500583660539'],\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Merchant City',\n",
       "   'data': ['Elk Grove', 'ONLINE', 'Pearland', 'Bowling Green', 'Fresno'],\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Merchant State',\n",
       "   'data': ['CA', 'nan', 'TX', 'KY', 'CA'],\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Zip',\n",
       "   'data': ['95624.0', 'nan', '77584.0', '42101.0', '93725.0'],\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'MCC',\n",
       "   'data': ['5541', '4784', '5912', '5311', '7995'],\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [5, 1]},\n",
       "  {'name': 'Errors?',\n",
       "   'data': ['nan', 'nan', 'nan', 'nan', 'Insufficient Balance'],\n",
       "   'datatype': 'BYTES',\n",
       "   'shape': [5, 1]}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebe886d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/octet-stream\", Body=json.dumps(payload)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f1c443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_body = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "predictions = response_body['outputs'][0]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eccd2739",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_LABELS = ['NOT FRAUD', 'FRAUD']\n",
    "predictions = [CLASS_LABELS[int(idx)] for idx in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5080852e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOT FRAUD', 'NOT FRAUD', 'NOT FRAUD', 'NOT FRAUD', 'NOT FRAUD']\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c707566",
   "metadata": {},
   "source": [
    "## Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d4fa690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'caed8352-bfaf-40d6-a3f2-4dc7fc63babd',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'caed8352-bfaf-40d6-a3f2-4dc7fc63babd',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Wed, 13 Jul 2022 01:54:19 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_model(ModelName=sm_model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
