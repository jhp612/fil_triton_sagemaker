{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "066c754c",
   "metadata": {},
   "source": [
    "# Triton on SageMaker - Ensemble + FIL Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9778812",
   "metadata": {},
   "source": [
    "## Set up Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938752bb",
   "metadata": {},
   "source": [
    "Installs the dependencies required to package the model and run inferences using Triton server.\n",
    "\n",
    "Also define the IAM role that will give SageMaker access to the model artifacts and the NVIDIA Triton ECR image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb84d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU pip awscli boto3 sagemaker \n",
    "!pip install nvidia-pyindex\n",
    "!pip install tritonclient[http]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd9fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "import time\n",
    "import os\n",
    "from sagemaker import get_execution_role\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07a7c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d09c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise(\"UNSUPPORTED REGION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c18ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "triton_image_uri = \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.05-py3\".format(\n",
    "    account_id=account_id_map[region], region=region, base=base\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6615545b",
   "metadata": {},
   "source": [
    "## Package models and dependencies and uploading to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb11f5b",
   "metadata": {},
   "source": [
    "First we create the Triton config file for the XGBoost model being served by the FIL Backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438da239",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "FIL_MODEL_DIR = './model_repository/fil'\n",
    "\n",
    "# Maximum size in bytes for input and output arrays\n",
    "MAX_MEMORY_BYTES = 60_000_000\n",
    "NUM_FEATURES = 15\n",
    "NUM_CLASSES = 2\n",
    "bytes_per_sample = (NUM_FEATURES + NUM_CLASSES) * 4\n",
    "max_batch_size = MAX_MEMORY_BYTES // bytes_per_sample\n",
    "\n",
    "IS_CLASSIFIER = True\n",
    "model_format = 'xgboost_json'\n",
    "\n",
    "# Select deployment hardware (GPU or CPU)\n",
    "if USE_GPU:\n",
    "    instance_kind = 'KIND_GPU'\n",
    "else:\n",
    "    instance_kind = 'KIND_CPU'\n",
    "\n",
    "# whether the model is doing classification or regression    \n",
    "if IS_CLASSIFIER:\n",
    "    classifier_string = 'true'\n",
    "else:\n",
    "    classifier_string = 'false'\n",
    "\n",
    "# whether to predict probabilites or not\n",
    "predict_proba = False\n",
    "\n",
    "if predict_proba:\n",
    "    predict_proba_string = 'true'\n",
    "else:\n",
    "    predict_proba_string = 'false'\n",
    "\n",
    "config_text = f\"\"\"backend: \"fil\"\n",
    "max_batch_size: {max_batch_size}\n",
    "input [                                 \n",
    " {{  \n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {NUM_FEATURES} ]                    \n",
    "  }} \n",
    "]\n",
    "output [\n",
    " {{\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "  }}\n",
    "]\n",
    "instance_group [{{ kind: {instance_kind} }}]\n",
    "parameters [\n",
    "  {{\n",
    "    key: \"model_type\"\n",
    "    value: {{ string_value: \"{model_format}\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"predict_proba\"\n",
    "    value: {{ string_value: \"{predict_proba_string}\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"output_class\"\n",
    "    value: {{ string_value: \"{classifier_string}\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"threshold\"\n",
    "    value: {{ string_value: \"0.5\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"storage_type\"\n",
    "    value: {{ string_value: \"AUTO\" }}\n",
    "  }}\n",
    "]\n",
    "\n",
    "dynamic_batching {{}}\"\"\"\n",
    "\n",
    "config_path = os.path.join(FIL_MODEL_DIR, 'config.pbtxt')\n",
    "with open(config_path, 'w') as file_:\n",
    "    file_.write(config_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91fb5dd",
   "metadata": {},
   "source": [
    "We follow the instructions [here](https://github.com/triton-inference-server/python_backend#2-packaging-the-conda-environment) for packaging preprocessing dependencies (here scikit-learn and pandas) to be used in the python backend as conda env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec228000",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash create_prep_env.sh\n",
    "!cp preprocessing_env.tar.gz model_repository/preprocessing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca9f0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move label encoders into python preprocessing directory\n",
    "!cp label_encoders.pkl model_repository/preprocessing/1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8914b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move trained xgboost model into fil model directory\n",
    "!mkdir -p model_repository/fil/1\n",
    "!cp xgboost.json model_repository/fil/1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131bc02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model version directory for ensemble model\n",
    "!mkdir -p model_repository/ensemble/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f95c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar --exclude='.ipynb_checkpoints' -czvf model.tar.gz -C model_repository ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf7e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = sagemaker_session.upload_data(path=\"model.tar.gz\", key_prefix=\"triton-fil-ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30226525",
   "metadata": {},
   "source": [
    "## Create SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c4acad",
   "metadata": {},
   "source": [
    "We start off by creating a sagemaker model from the model files we uploaded to s3 in the previous step.\n",
    "\n",
    "In this step we also provide an additional Environment Variable i.e. `SAGEMAKER_TRITON_DEFAULT_MODEL_NAME` which specifies the name of the model to be loaded by Triton. **The value of this key should match the folder name in the model package uploaded to s3.** This variable is optional in case of a single model. In case of ensemble models, this **key has to be specified** for Triton to startup in SageMaker.\n",
    "\n",
    "Additionally, customers can set `SAGEMAKER_TRITON_BUFFER_MANAGER_THREAD_COUNT` and `SAGEMAKER_TRITON_THREAD_COUNT` for optimizing the thread counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c792d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model_name = \"triton-fil-ensemble-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "container = {\n",
    "    \"Image\": triton_image_uri,\n",
    "    \"ModelDataUrl\": model_uri,\n",
    "    \"Environment\": {\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": \"ensemble\"},\n",
    "}\n",
    "\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e81fa4d",
   "metadata": {},
   "source": [
    "Using the model above, we create an endpoint configuration where we can specify the type and number of instances we want in the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935f6443",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = \"triton-fil-ensemble-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g4dn.4xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834047e7",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to InService once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec2ab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"triton-fil-ensemble-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fe1971",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72665d7e",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea539d1",
   "metadata": {},
   "source": [
    "Once we have the endpoint running we can use some sample raw data to do an inference using json as the payload format. For inference request format, Triton uses the KFServing community standard [inference protocols.](https://github.com/triton-inference-server/server/blob/main/docs/protocol/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a2ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_infer = pd.read_csv(\"data_infer.csv\")\n",
    "data_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc92ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "STR_COLUMNS = ['Time',\n",
    " 'Amount',\n",
    " 'Zip',\n",
    " 'MCC',\n",
    " 'Merchant Name',\n",
    " 'Use Chip',\n",
    " 'Merchant City',\n",
    " 'Merchant State',\n",
    " 'Errors?']\n",
    "\n",
    "batch_size = len(data_infer)\n",
    "\n",
    "payload = {}\n",
    "payload[\"inputs\"] = []\n",
    "data_dict = {}\n",
    "for col in data_infer.columns:\n",
    "    data_dict[col] = {}\n",
    "    data_dict[col]['name'] = col\n",
    "    if col in STR_COLUMNS:\n",
    "        data_dict[col]['data'] = data_infer[col].astype(str).tolist()\n",
    "        data_dict[col]['datatype'] = 'BYTES'\n",
    "    else:\n",
    "        data_dict[col]['data'] = data_infer[col].astype('float32').tolist()\n",
    "        data_dict[col]['datatype'] = 'FP32'\n",
    "    data_dict[col]['shape'] = [batch_size, 1]\n",
    "    payload[\"inputs\"].append(data_dict[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baae5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed44b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/octet-stream\", Body=json.dumps(payload)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36868819",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_body = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "predictions = response_body['outputs'][0]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0f4b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_LABELS = ['NOT FRAUD', 'FRAUD']\n",
    "predictions = [CLASS_LABELS[int(idx)] for idx in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0174b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e562ed0",
   "metadata": {},
   "source": [
    "## Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c21ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_model(ModelName=sm_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71442539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
